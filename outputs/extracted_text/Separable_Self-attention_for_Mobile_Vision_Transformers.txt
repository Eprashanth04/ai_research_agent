--- Page 1 ---
Separable Self-attention for Mobile Vision
Transformers
Sachin Mehta
AppleMohammad Rastegari
Apple
Abstract
Mobile vision transformers (MobileViT) can achieve state-of-the-art performance
across several mobile vision tasks, including classiﬁcation and detection. Though
these models have fewer parameters, they have high latency as compared to con-
volutional neural network-based models. The main efﬁciency bottleneck in Mo-
bileViT is the multi-headed self-attention (MHA) in transformers, which requires
O(k2)time complexity with respect to the number of tokens (or patches) k. More-
over, MHA requires costly operations (e.g., batch-wise matrix multiplication)
for computing self-attention, impacting latency on resource-constrained devices.
This paper introduces a separable self-attention method with linear complexity,
i.e.O(k). A simple yet effective characteristic of the proposed method is that
it uses element-wise operations for computing self-attention, making it a good
choice for resource-constrained devices. The improved model, MobileViTv2 , is
state-of-the-art on several mobile vision tasks, including ImageNet object classi-
ﬁcation and MS-COCO object detection. With about three million parameters,
MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, out-
performing MobileViT by about 1% while running 3:2faster on a mobile device.
Our source code is available at: https://github.com/apple/ml-cvnets
1 Introduction
Vision transformers (ViTs) [ 1] have become ubiquitous for a wide variety of visual recognition tasks
[2,3], including mobile vision tasks [ 4]. At the heart of the ViT-based models, including mobile
vision transformers, is the transformer block [ 5]. The main efﬁciency bottleneck in ViT-based models,
especially for inference on resource-constrained devices, is the multi-headed self-attention (MHA).
MHA allows the tokens (or patches) to interact with each other, and is a key for learning global
representations. However, the complexity of self-attention in transformer block is O(k2), i.e., it is
quadratic with respect to the number of tokens (or patches) k. Besides this, computationally expensive
operations (e.g., batch-wise matrix multiplication; see Fig. 1) are required to compute attention matrix
in MHA. This, in particular, is concerning for deploying ViT-based models on resource-constrained
devices, as these devices have reduced computational capabilities, restrictive memory constraints,
and a limited power budget. Therefore, this paper seeks to answer this question: can self-attention in
transformer block be optimized for resource-constrained devices?
Several methods [e.g., 7–10] have been proposed for optimizing the self-attention operation in
transformers (not necessarily for ViTs). Among these, a widely studied approach in sequence
modeling tasks is to introduce sparsity in self-attention layers, wherein each token attends to a
subset of tokens in an input sequence [ 7,9]. Though these approaches reduces the time complexity
fromO(k2)toO(kp
k)orO(klogk), the cost is a performance drop. Another popular approach
for approximating self-attention is via low-rank approximation. Linformer [ 10] decomposes the
self-attention operation into multiple smaller self-attention operations via linear projections, and
reduces the complexity of self-attention from O(k2)toO(k). However, Linformer still uses costly
Preprint. Under review.arXiv:2206.02680v1  [cs.CV]  6 Jun 2022
--- Page 2 ---
Transformer (Total time = 12.3 ms)
Linformer (Total time = 13.8 ms)
Ours (Total time = 7.7 ms)Top-5 operations in each layerTheoretical time complexity
CPU latency vs. Tokens
<latexit sha1_base64="I1wmkFxj3dvBneHHWWVgR2fppBg=">AAADEnicbVLLbtNAFB2bVwmvAEs2I1JQ2UR2VB7Lim5YgFqkpq0Uh2g8vk5HmYc1cw2NLH8AW+Br2CG2/AAfA2KSWqFOe6WRjs6599y5dyYtpHAYRb+D8MrVa9dvbNzs3Lp95+697v0Hh86UlsOQG2nsccocSKFhiAIlHBcWmEolHKWz3YV+9BGsE0Yf4LyAsWJTLXLBGXpq0v2TpDAVukKWlpLZupK87tAmEjSFLSWM4v7zAscr3gtwimlevTMZyJo+XRG7RhUSTgXOa5+V/HdSImucBi2rA8u0y41VYL0N3dzbmn0YPNs8X/tW6FXCQm+re6V1lwpJahCNWr9/AjpbTTvp9qJ+tAx6EcQN6JEm9ifdv0lmeKlAI5fMuVEceeOKWRRcQt1JSgcF4zM2hZGHmilw42r5TDV94pmM+ln80UiX7PmKiinn5ir1mYrhiVvXFuRl2qjE/NW4ErooETQ/a5SXkqKhizenmbDAUc49YNwKf1fKT5hlHP3PaHVJjZn55bjWJJUqJQprPtUdv7B4fT0XweGgH7/ob7/f7u28bla3QR6Rx2SLxOQl2SFvyD4ZEh7w4HPwJfgafgu/hz/Cn2epYdDUPCStCH/9A6XA6Yo=</latexit>Model ComplexityTransformerO(k2)LinformerO(k)OursO(k)
<latexit sha1_base64="FKPOTpWmrtIx/FBC+Zno4CdIQlc=">AAAECHicjVPLbtNAFJ3aPNrwaApLNiMiEBssO21C6aqCDRKqVKSmrRRH0Xh8k44y47HmgYgs/wAfwBY+gR1iy1/wBfwG48StaBIIV7J0de8599w540lyzrQJw58bnn/j5q3bm1uNO3fv3d9u7jw41dIqCj0quVTnCdHAWQY9wwyH81wBEQmHs2TyuuqfvQelmcxOzDSHgSDjjI0YJcaVhjveVpzAmGWFIYnlRJUFV6ps4IWIjcyV5dCPgk5uBiv68MEko+JE5s87WOagZvN1iZ/iWFhumFvViqyIyoKWxRWcCSjXYI5ASDV1qDhelhUsrddqr9wLEwPZwYEQTqMb7GKhXeLOgI+SVfNqOElTB2t32kGI7RrGyGpI46ECbuOhO0WFjnYviZ2omvH2X2LaVsu9jC4pawlzkf3u/2jEiTRGir9dXQxZenXzw2YrDMJZ4OUkqpMWquN42PwVp5JaAZmhnGjdj0InUBDlbpJD2YidNTmhEzKGvkszIkAPitk/W+InlW94JJX7MoNn1T8ZBRFaT0XikIKYC73Yq4qren1rRvuDgmW5dW7RudDIWWYkrh4ATpkCavjUJYQq5nbF9IIoQo17JtdUEiknzhxdORMt+rCcnLaDqBvsvdtrHb6qPdpEj9Bj9AxF6AU6RG/QMeoh6uXeJ++z98X/6H/1v/nf51Bvo+Y8RNfC//EbnjsgRg==</latexit>Top-5 operations Time Memoryaten::mm 6.3 ms 1.5 Mbaten::add 252.0 us 1.5 Mbfusedrelumul 113.0 us 512.0 Kbaten::sum 91.0 us 2.0 Kbaten::mul 86.0 us 512.0 Kb
<latexit sha1_base64="K7ZmNecfsZiUaO78zFC9KeVDASI=">AAAEW3ichVNNb9MwGHbTAqMMtoI4wcGiYuIAVVJaKDtN4wCXSUNat0lNVdmO01n1R2Q7o1WUE7+GK/waDvwX3DSd6Mc0S5EevR/P876PY5xwZqzv/6l41dq9+w92HtYf7T5+srffeHpuVKoJ7RPFlb7EyFDOJO1bZjm9TDRFAnN6gSef5/mLa6oNU/LMzhI6FGgsWcwIsi40angvQ0zHTGYW4ZQjnWdc67wO105oVaJTTgdBq5vY4ZY8nVocZ2cqedeFKqG64Dc5PIChSLllbtRUyCzIM5JnN+VM0PyOmhMqlJ65qjDclBUsKsdqb50LIkvl4aEQTqPX8qEwDrQdOMHb+GCo1ffC1OyL4hGVWkV5yYELkqD1aUnSvY1kUR+OjIqtQFNX/L7bcZrpHeKLPqKSWTiaN/ndZZN/VxOKojl3t7cic7zVM6ysVeK22wypjG5+htF+00kXB26CoARNUJ7TUaNyEEaKpIJKSzgyZhD4TiFD2t0up3k9TA1NEJmgMR04KJGgZpgVlufwtYtEMFbafdLCIvp/R4aEMTOBXaVA9sqs5+bBbblBauPeMGMySZ1bZCEUpxxaBeePAkZMU2L5zAFENHOzQnKFNCLWPZ0VFazUxLljVjZZ+jV9W4xsMV9dNLpmiSlXnS52dc4G6z5ugvN2K/jQ6nzrNI+OS493wAvwCrwBAfgIjsBXcAr6gHg/vJ/eL+939W+tWqvXdhelXqXseQZWTu35P4syPuE=</latexit>Top-5 operations Time Memoryaten::mm 8.0 ms 2.0 Mbaten::bmm1.9 ms2.5 Mbaten::softmax 354.0 us 2.0 Mbaten::copy305.0 us 0.0 Mbaten::add 258.0 us 2.0 MB
<latexit sha1_base64="D25nWpdJ7G+zuU9bp5dS3vuXcNI=">AAAEW3icjVPLbtNAFHWcACUUaECsYDEiomIBlp00aemqggVsKhWpLymOopnxOB1lHtbMuCSyvOJr2MLXsOBfGDtuRV4qI1m6uvfcc+4940EJo9r4/u+aW2/cu/9g62Hz0fbjJ093Ws/OtUwVJmdYMqkuEdSEUUHODDWMXCaKQI4YuUCTT0X94pooTaU4NbOEDDkcCxpTDI1NjVruqxCRMRWZgShlUOUZUypvgqUTGpmolJFB4PUSM1xTJ1OD4uxUJu97QCZElfw6B7sg5Ckz1I6acpEFeYbz7BZOOcnvwBwTLtXMosJwVZbTqBqrs3YuAA0Rh4ecW4197wBwbYOO54NjtI4PhEp+K03NPksWEaFklFccqCTpev05SdfrbSKZ48ORlrHhcFqAfd9qpneIV7OmrIB96P93B6FCp8VwQb970+RvbAqRNEbyTbcZEhHd/gyjnbYlKg9YDYIqaDvVORm1arthJHHKiTCYQa0HgW8VMqjs7TKSN8NUkwTiCRyTgQ0F5EQPs9LyHLyxmQjEUtlPGFBm/+3IINd6xpFFcmiu9HKtSK6rDVITHwwzKpLUGobnQrH12UhQPAoQUUWwYTMbQKyonRXgK6ggNvbpLKggKSfWHb2wyY1f03flyAaxxUWja5roatXpfFfrbLDs42pw3vGCvrf3da999LHyeMt56bx23jqBs+8cOV+cE+fMwe5394f70/1V/9OoN5qN7TnUrVU9z52F03jxF0jMP0M=</latexit>Top-5 operations Time Memoryaten::mm 7.8 ms 2.0 Mbaten::bmm3.6 ms3.5 Mbaten::softmax 300.0 us 2.0 Mbaten::mul 296.0 us 2.0 Mbaten::einsum 163.0 us 0.0 MbFigure 1: Comparison between different attention units. Transformer and Linformer use costly
operations (batch-wise matrix multiplication) for computing self-attention. Such operations are a
bottleneck for efﬁcient inference on resource-constrained devices. The proposed method does not use
such operations, thus accelerating inference on resource-constrained devices. Left compares top-5
operations (sorted by CPU time) in a single layer of different attention units for k= 256 tokens.
Top Right compares complexity of different attention units. Bottom Right compares the latency
of different attention units as a function of the number of tokens k. These results are computed
on a single CPU core machine with a 2.4 GHz 8-Core Intel Core i9 processor, d= 512 (token
dimensionality), h= 8(number of heads; for Transformer and Linformer), and p= 256 (projected
tokens in Linformer) using a publicly available proﬁler in PyTorch [6].
1 2 3 4 5 6 7 8 9 10
Inference time (in ms)6870727476788082T op-1 accuracy 
 (in %)
 6.4%
better
3.2× faster
MobileViTv1
MobileViTv2
(a) ImageNet-1k classiﬁcation
2 4 6 8 10 12 14 16 18
Inference time (in ms)202224262830Mean Average Precision 
 (in %)
 5.4%
better
3× faster
MobileViTv1
MobileViTv2 (b) MS-COCO object detection
5 10 15 20 25 30 35 40
Inference time (in ms)737475767778798081Mean Intersection over Union 
 (in %)
 6.7% 
better
3.1× fasterMobileViTv1
MobileViTv2 (c) PASCAL VOC segmentation
Figure 2: MobileViTv2 models are faster and better than MobileViTv1 models [4] across dif-
ferent tasks. MobileViTv2 models are constructed by replacing multi-headed self-attention in
MobileViTv1 with the proposed separable self-attention (Section 3.2). Here, inference time is
measured on an iPhone12 for an input resolution of 256256,512512, and 320320for
classiﬁcation, segmentation, and detection respectively.
operations (e.g., batch-wise matrix multiplication; Fig. 1) for learning global representations in MHA,
which may hinder the deployment of these models on resource-constrained devices.
This paper introduces a novel method, separable self-attention , withO(k)complexity for addressing
the bottlenecks in MHA in transformers. For efﬁcient inference, the proposed self-attention method
also replaces the computationally expensive operations (e.g., batch-wise matrix multiplication) in
MHA with element-wise operations (e.g., summation and multiplication). Experimental results on
standard vision datasets and tasks demonstrates the effectiveness of the proposed method (Fig. 2).
2
--- Page 3 ---
2 Related work
Improving self-attention Improving the efﬁciency of MHA in transformers is an active area of
research. The ﬁrst line of research introduces locality to address the computational bottleneck in
MHA [e.g., 7,9,11,12]. Instead of attending to all ktokens, these methods use predeﬁned patterns
to limit the receptive ﬁeld of self-attention from all ktokens to a subset of tokens, reducing the
time complexity from O(k2)toO(kp
k)orO(klogk). However, such methods suffer from large
performance degradation with moderate training/inference speed-up over the standard MHA in
transformers. To improve the efﬁciency of MHA, the second line of research uses similarity measures
to group tokens [ 8,13,14]. For instance, Reformer [ 8] uses locality-sensitive hashing to group
the tokens and reduces the theoretical self-attention cost from O(k2)toO(klogk). However, the
efﬁciency gains over standard MHA are noticeable only for large sequences ( k>2048 ) [8]. Because
k<1024 in ViTs, these approaches are not suitable for ViTs. The third line of research improves
the efﬁciency of MHA via low-rank approximation [ 10,15]. The main idea is to approximate
the self-attention matrix with a low-rank matrix, reducing the computational cost from O(k2)to
O(k). Even though these methods speed-up the self-attention operation signiﬁcantly, they still use
expensive operations for computing attention, which may hinder the deployment of these models on
resource-constrained devices (Fig. 1).
In summary, existing methods for improving MHA are limited in their reduction of inference time
and memory consumption, especially for resource-constrained devices. This work introduces a
separable self-attention method that is fast and memory-efﬁcient (see Fig. 1), which is desirable for
resource-constrained devices.
Improving transformer-based models There has been signiﬁcant work on improving the efﬁ-
ciency of transformers [ 3,4,16–18]. The majority of these approaches reduce the number of tokens
in the transformer block using different methods, including down-sampling [ 19,18] and pyramidal
structure [ 3,20,4]. Because the proposed separable self-attention module is a drop-in replacement to
MHA, it can be easily integrated with any transformer-based model to further improve its efﬁciency.
Other methods Transformer-based models performance can be improved using different methods,
including mixed-precision training [ 21], efﬁcient optimizers [ 22,23], and knowledge distillation [ 2].
These methods are orthogonal to our work, and by default, we use mixed-precision during training.
3MobileViTv2
MobileViT [ 4] is a hybrid network that combines the strengths of CNNs and ViTs. MobileViT
views transformers as convolutions, which allows it to leverage the merits of both convolutions (e.g.,
inductive biases) and transformers (e.g., long-range dependencies) to build a light-weight network for
mobile devices. Though MobileViT networks have signiﬁcantly fewer parameters and deliver better
performance as compared to light-weight CNNs (e.g., MobileNets [ 24,25]), they have high latency.
The main efﬁciency bottleneck in MobileViT is the multi-headed self-attention (MHA; Fig. 3a).
MHA uses scaled dot-product attention to capture the contextual relationships between ktokens
(or patches). However, MHA is expensive as it has O(k2)time complexity. This quadratic cost
is a bottleneck for transformers with a large number of tokens k(Fig. 1). Moreover, MHA uses
computationally- and memory-intensive operations (e.g., batch-wise matrix multiplication and soft-
max for computing attention matrix; Fig. 1); which could be a bottleneck on resource-constrained
devices. To address the limitations of MHA for efﬁcient inference on resource-constrained devices,
this paper introduces separable self-attention with linear complexity (Fig. 3c).
The main idea of our separable self-attention approach, shown in Fig. 4b, is to compute context scores
with respect to a latent token L. These scores are then used to re-weight the input tokens and produce
a context vector, which encodes the global information. Because the self-attention is computed with
respect to a latent token, the proposed method can reduce the complexity of self-attention in the
transformer by a factor k. A simple yet effective characteristic of the proposed method is that it
uses element-wise operations (e.g., summation and multiplication) for its implementation, making it
a good choice for resource-constrained devices. We call the proposed attention method separable
self-attention because it allows us to encode global information by replacing the quadratic MHA
3
--- Page 4 ---
Broadcasted element-wise multiplication<latexit sha1_base64="r3RhxPibzptum5p4iWVmnNh7d98=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzAOSJcxOZpMhM7PLPISw5Be8eFDEqz/kzb9xNtmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtnVhFaIskPFHdCGvKmaQtwwyn3VRRLCJOO9HkLvc7T1RplshHM01pKPBIspgRbHKpr60YVGt+3Z8DrZKgIDUo0BxUv/rDhFhBpSEca90L/NSEGVaGEU5nlb7VNMVkgke056jEguowm986Q2dOGaI4Ua6kQXP190SGhdZTEblOgc1YL3u5+J/Xsya+CTMmU2uoJItFseXIJCh/HA2ZosTwqSOYKOZuRWSMFSbGxVNxIQTLL6+S9kU9uKoHD5e1xm0RRxlO4BTOIYBraMA9NKEFBMbwDK/w5gnvxXv3PhatJa+YOYY/8D5/ADPjjlk=</latexit>XElement-wise sum<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> SoftmaxDot-productConcatenation
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinearLinearLinearLinearLinearLinear
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dhLinearLinearLinear<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> <latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
LinearAttention matrix<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dTranspose<latexit sha1_base64="gMTgjs7J9T7tfl8I4J/4iGZ8J/U=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APbUjLpnTY0kxmSjFiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybnHjwXXxnW/ncLK6tr6RnGztLW9s7tX3j9o6ihRDBssEpFq+1Sj4BIbhhuB7VghDX2BLX98k/mtR1SaR/LeTGLshXQoecAZNVZ66IbUjPwgfZr2yxW36s5AlomXkwrkqPfLX91BxJIQpWGCat3x3Nj0UqoMZwKnpW6iMaZsTIfYsVTSEHUvnSWekhOrDEgQKfukITP190ZKQ60noW8ns4R60cvE/7xOYoKrXsplnBiUbP5RkAhiIpKdTwZcITNiYgllitushI2ooszYkkq2BG/x5GXSPKt6F9Xzu/NK7TqvowhHcAyn4MEl1OAW6tAABhKe4RXeHO28OO/Ox3y04OQ7h/AHzucPADaRJQ==</latexit>x
<latexit sha1_base64="Orsd8Uetr8OD0z/KWA5V57CS7gM=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae2Q8mkmTY0kwxJRhiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybkniDnTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkoQttEcql6AdaUM0HbhhlOe7GiOAo47QbT29zvPlGlmRQPJo2pH+GxYCEj2FjpcRBhMwnCLJ0NqzW37s6BVolXkBoUaA2rX4ORJElEhSEca9333Nj4GVaGEU5nlUGiaYzJFI9p31KBI6r9bJ54hs6sMkKhVPYJg+bq740MR1qnUWAn84R62cvF/7x+YsJrP2MiTgwVZPFRmHBkJMrPRyOmKDE8tQQTxWxWRCZYYWJsSRVbgrd88irpXNS9y3rjvlFr3hR1lOEETuEcPLiCJtxBC9pAQMAzvMKbo50X5935WIyWnGLnGP7A+fwBAbuRJg==</latexit>y<latexit sha1_base64="LbIiugsfjMiuQbZM+HeSITVF930=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQxjIB84BsCLOTu8mQ2Qczd8WwbGPjr9hYKGLrP9j5N85uUmjigYHDOfc1x40EV2hZ30ZhZXVtfaO4Wdra3tndK+8ftFUYSwYtFopQdl2qQPAAWshRQDeSQH1XQMed3GR+5x6k4mFwh9MI+j4dBdzjjKKWBuVjB+EB8zmJhGGaOD7FMaMiaabpoFyxqlYOc5nYc1IhczQG5S9nGLLYhwCZoEr1bCvCfkIlciYgLTmxgoiyCR1BT9OA+qD6Sb49NU+1MjS9UOoXoJmrvzsS6is19V1dmd2oFr1M/M/rxehd9RMeRDFCwGaLvFiYGJpZJOaQS2AopppQJrm+1WRjKilDHVxJh2AvfnmZtM+r9kW11qxV6tfzOIrkiJyQM2KTS1Int6RBWoSRR/JMXsmb8WS8GO/Gx6y0YMx7DskfGJ8/RAaZtg==</latexit>Q<latexit sha1_base64="b2P15Xg35B2Uc1NdmMfGE2pXebI=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQRrCJYB6QXcLs5CYZMvtg5q4Ylm1s/BUbC0Vs/Qc7/8bJZgtNPDBwOOe+5niR4Aot69soLC2vrK4V10sbm1vbO+XdvZYKY8mgyUIRyo5HFQgeQBM5CuhEEqjvCWh746up374HqXgY3OEkAtenw4APOKOopV750EF4wGxOIqGfJo5PccSoSG7StFeuWFUrg7lI7JxUSI5Gr/zl9EMW+xAgE1Sprm1F6CZUImcC0pITK4goG9MhdDUNqA/KTbLtqXmslb45CKV+AZqZ+rsjob5SE9/TldMb1bw3Ff/zujEOLtyEB1GMELDZokEsTAzNaSRmn0tgKCaaUCa5vtVkIyopQx1cSYdgz395kbROq/ZZtXZbq9Qv8ziK5IAckRNik3NSJ9ekQZqEkUfyTF7Jm/FkvBjvxsestGDkPfvkD4zPHzrimbA=</latexit>K<latexit sha1_base64="Ta0kqL4N5jGMEBTHhbK9R3DfFLU=">AAACBXicbVDLSsNAFJ34rPVVdamLYBFclUSKuiy6cVnBPqAtZTK5aYdOJmHmRiwhGzf+ihsXirj1H9z5N07bLLT1wMDhnPua48WCa3Scb2tpeWV1bb2wUdzc2t7ZLe3tN3WUKAYNFolItT2qQXAJDeQooB0roKEnoOWNrid+6x6U5pG8w3EMvZAOJA84o2ikfumoi/CA0zmpAj9LuyHFIaMibWZZv1R2Ks4U9iJxc1ImOer90lfXj1gSgkQmqNYd14mxl1KFnAnIit1EQ0zZiA6gY6ikIeheOt2e2SdG8e0gUuZJtKfq746UhlqPQ89UTm7U895E/M/rJBhc9lIu4wRBstmiIBE2RvYkEtvnChiKsSGUKW5utdmQKsrQBFc0IbjzX14kzbOKe16p3lbLtas8jgI5JMfklLjkgtTIDamTBmHkkTyTV/JmPVkv1rv1MStdsvKeA/IH1ucPS6SZuw==</latexit>V(a) MHA in Transformers [5]
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinearLinearLinearLinearLinearLinear
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>hLinearLinearLinear
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> <latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
LinearAttention matrix<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="1JD4AJrhWi5kJ4jrLD6rC4H0WMQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPXLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDz/GM8g==</latexit>h<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dTranspose<latexit sha1_base64="gMTgjs7J9T7tfl8I4J/4iGZ8J/U=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APbUjLpnTY0kxmSjFiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybnHjwXXxnW/ncLK6tr6RnGztLW9s7tX3j9o6ihRDBssEpFq+1Sj4BIbhhuB7VghDX2BLX98k/mtR1SaR/LeTGLshXQoecAZNVZ66IbUjPwgfZr2yxW36s5AlomXkwrkqPfLX91BxJIQpWGCat3x3Nj0UqoMZwKnpW6iMaZsTIfYsVTSEHUvnSWekhOrDEgQKfukITP190ZKQ60noW8ns4R60cvE/7xOYoKrXsplnBiUbP5RkAhiIpKdTwZcITNiYgllitushI2ooszYkkq2BG/x5GXSPKt6F9Xzu/NK7TqvowhHcAyn4MEl1OAW6tAABhKe4RXeHO28OO/Ox3y04OQ7h/AHzucPADaRJQ==</latexit>x
<latexit sha1_base64="Orsd8Uetr8OD0z/KWA5V57CS7gM=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae2Q8mkmTY0kwxJRhiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybkniDnTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkoQttEcql6AdaUM0HbhhlOe7GiOAo47QbT29zvPlGlmRQPJo2pH+GxYCEj2FjpcRBhMwnCLJ0NqzW37s6BVolXkBoUaA2rX4ORJElEhSEca9333Nj4GVaGEU5nlUGiaYzJFI9p31KBI6r9bJ54hs6sMkKhVPYJg+bq740MR1qnUWAn84R62cvF/7x+YsJrP2MiTgwVZPFRmHBkJMrPRyOmKDE8tQQTxWxWRCZYYWJsSRVbgrd88irpXNS9y3rjvlFr3hR1lOEETuEcPLiCJtxBC9pAQMAzvMKbo50X5935WIyWnGLnGP7A+fwBAbuRJg==</latexit>y<latexit sha1_base64="LbIiugsfjMiuQbZM+HeSITVF930=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQxjIB84BsCLOTu8mQ2Qczd8WwbGPjr9hYKGLrP9j5N85uUmjigYHDOfc1x40EV2hZ30ZhZXVtfaO4Wdra3tndK+8ftFUYSwYtFopQdl2qQPAAWshRQDeSQH1XQMed3GR+5x6k4mFwh9MI+j4dBdzjjKKWBuVjB+EB8zmJhGGaOD7FMaMiaabpoFyxqlYOc5nYc1IhczQG5S9nGLLYhwCZoEr1bCvCfkIlciYgLTmxgoiyCR1BT9OA+qD6Sb49NU+1MjS9UOoXoJmrvzsS6is19V1dmd2oFr1M/M/rxehd9RMeRDFCwGaLvFiYGJpZJOaQS2AopppQJrm+1WRjKilDHVxJh2AvfnmZtM+r9kW11qxV6tfzOIrkiJyQM2KTS1Int6RBWoSRR/JMXsmb8WS8GO/Gx6y0YMx7DskfGJ8/RAaZtg==</latexit>Q<latexit sha1_base64="b2P15Xg35B2Uc1NdmMfGE2pXebI=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQRrCJYB6QXcLs5CYZMvtg5q4Ylm1s/BUbC0Vs/Qc7/8bJZgtNPDBwOOe+5niR4Aot69soLC2vrK4V10sbm1vbO+XdvZYKY8mgyUIRyo5HFQgeQBM5CuhEEqjvCWh746up374HqXgY3OEkAtenw4APOKOopV750EF4wGxOIqGfJo5PccSoSG7StFeuWFUrg7lI7JxUSI5Gr/zl9EMW+xAgE1Sprm1F6CZUImcC0pITK4goG9MhdDUNqA/KTbLtqXmslb45CKV+AZqZ+rsjob5SE9/TldMb1bw3Ff/zujEOLtyEB1GMELDZokEsTAzNaSRmn0tgKCaaUCa5vtVkIyopQx1cSYdgz395kbROq/ZZtXZbq9Qv8ziK5IAckRNik3NSJ9ekQZqEkUfyTF7Jm/FkvBjvxsestGDkPfvkD4zPHzrimbA=</latexit>K<latexit sha1_base64="Ta0kqL4N5jGMEBTHhbK9R3DfFLU=">AAACBXicbVDLSsNAFJ34rPVVdamLYBFclUSKuiy6cVnBPqAtZTK5aYdOJmHmRiwhGzf+ihsXirj1H9z5N07bLLT1wMDhnPua48WCa3Scb2tpeWV1bb2wUdzc2t7ZLe3tN3WUKAYNFolItT2qQXAJDeQooB0roKEnoOWNrid+6x6U5pG8w3EMvZAOJA84o2ikfumoi/CA0zmpAj9LuyHFIaMibWZZv1R2Ks4U9iJxc1ImOer90lfXj1gSgkQmqNYd14mxl1KFnAnIit1EQ0zZiA6gY6ikIeheOt2e2SdG8e0gUuZJtKfq746UhlqPQ89UTm7U895E/M/rJBhc9lIu4wRBstmiIBE2RvYkEtvnChiKsSGUKW5utdmQKsrQBFc0IbjzX14kzbOKe16p3lbLtas8jgI5JMfklLjkgtTIDamTBmHkkTyTV/JmPVkv1rv1MStdsvKeA/IH1ucPS6SZuw==</latexit>V
<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="D9dv/kQhc4rGfkzamaQeeya8K1s=">AAACPXicbVDLSsNAFJ34rPXV6tJNsCgupCRS1GXRjcsW7APaUCaTSTt0kgkzN6Ul9Avc6vf4HX6AO3Hr1mmahWm9cOFw7r3cc44bcabAsj6Mjc2t7Z3dwl5x/+Dw6LhUPmkrEUtCW0RwIbsuVpSzkLaAAafdSFIcuJx23PHjYt6ZUKmYCJ9hFlEnwMOQ+Yxg0FQzGpQqVtVKy1wHdgYqKKvGoGxc9j1B4oCGQDhWqmdbETgJlsAIp/NiP1Y0wmSMh7SnYYgDqpwkVTo3LzTjmb6QukMwU/bvRYIDpWaBqzcDDCO1OluQ/816Mfj3TsLCKAYakuUjP+YmCHNh2/SYpAT4TANMJNNaTTLCEhPQ4eS+uEKMAbsq5yTRTMyxnF6nksHleaPehEUqszpdetXJ2qs5roP2TdW+rdaatUr9Icu4gM7QObpCNrpDdfSEGqiFCKLoBb2iN+Pd+DS+jO/l6oaR3ZyiXBk/v0SrsCU=</latexit>pToken ProtectionToken ProtectionToken Projection<latexit sha1_base64="kKwSuyC1ChAX9ZkJxYFeJUKsB40=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK1hbaUDabTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmlldW19o7xZ2dre2d2r7h88miTTjLdYIhPdCajhUijeQoGSd1LNaRxI3g5GN1O//cS1EYl6wHHK/ZgOlIgEo2il+7A/7Fdrbt2dgSwTryA1KNDsV796YcKymCtkkhrT9dwU/ZxqFEzySaWXGZ5SNqID3rVU0ZgbP5+dOiEnVglJlGhbCslM/T2R09iYcRzYzpji0Cx6U/E/r5thdOXnQqUZcsXmi6JMEkzI9G8SCs0ZyrEllGlhbyVsSDVlaNOp2BC8xZeXyeNZ3buoe3fntcZ1EUcZjuAYTsGDS2jALTShBQwG8Ayv8OZI58V5dz7mrSWnmDmEP3A+fwBCjI3J</latexit>dh
<latexit sha1_base64="gRkLF/8GQhmCxuxt1NbQbx39AfI=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF49V7Ae0oWy2k3bpZhN2N0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz00Mv65Ypbdecgq8TLSQVyNPrlr94gZmmE0jBBte56bmL8jCrDmcBpqZdqTCgb0yF2LZU0Qu1n80un5MwqAxLGypY0ZK7+nshopPUkCmxnRM1IL3sz8T+vm5rw2s+4TFKDki0WhakgJiazt8mAK2RGTCyhTHF7K2EjqigzNpySDcFbfnmVtC6qXq3q3V9W6jd5HEU4gVM4Bw+uoA530IAmMAjhGV7hzRk7L86787FoLTj5zDH8gfP5A518jWs=</latexit>{<latexit sha1_base64="D9dv/kQhc4rGfkzamaQeeya8K1s=">AAACPXicbVDLSsNAFJ34rPXV6tJNsCgupCRS1GXRjcsW7APaUCaTSTt0kgkzN6Ul9Avc6vf4HX6AO3Hr1mmahWm9cOFw7r3cc44bcabAsj6Mjc2t7Z3dwl5x/+Dw6LhUPmkrEUtCW0RwIbsuVpSzkLaAAafdSFIcuJx23PHjYt6ZUKmYCJ9hFlEnwMOQ+Yxg0FQzGpQqVtVKy1wHdgYqKKvGoGxc9j1B4oCGQDhWqmdbETgJlsAIp/NiP1Y0wmSMh7SnYYgDqpwkVTo3LzTjmb6QukMwU/bvRYIDpWaBqzcDDCO1OluQ/816Mfj3TsLCKAYakuUjP+YmCHNh2/SYpAT4TANMJNNaTTLCEhPQ4eS+uEKMAbsq5yTRTMyxnF6nksHleaPehEUqszpdetXJ2qs5roP2TdW+rdaatUr9Icu4gM7QObpCNrpDdfSEGqiFCKLoBb2iN+Pd+DS+jO/l6oaR3ZyiXBk/v0SrsCU=</latexit>pToken ProtectionToken ProtectionToken Projection
<latexit sha1_base64="D9dv/kQhc4rGfkzamaQeeya8K1s=">AAACPXicbVDLSsNAFJ34rPXV6tJNsCgupCRS1GXRjcsW7APaUCaTSTt0kgkzN6Ul9Avc6vf4HX6AO3Hr1mmahWm9cOFw7r3cc44bcabAsj6Mjc2t7Z3dwl5x/+Dw6LhUPmkrEUtCW0RwIbsuVpSzkLaAAafdSFIcuJx23PHjYt6ZUKmYCJ9hFlEnwMOQ+Yxg0FQzGpQqVtVKy1wHdgYqKKvGoGxc9j1B4oCGQDhWqmdbETgJlsAIp/NiP1Y0wmSMh7SnYYgDqpwkVTo3LzTjmb6QukMwU/bvRYIDpWaBqzcDDCO1OluQ/816Mfj3TsLCKAYakuUjP+YmCHNh2/SYpAT4TANMJNNaTTLCEhPQ4eS+uEKMAbsq5yTRTMyxnF6nksHleaPehEUqszpdetXJ2qs5roP2TdW+rdaatUr9Icu4gM7QObpCNrpDdfSEGqiFCKLoBb2iN+Pd+DS+jO/l6oaR3ZyiXBk/v0SrsCU=</latexit>p (b) MHA in Linformer [10]
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinearLinear
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>kContext  vectorLinear
<latexit sha1_base64="N/EBWZoOVLkLENSRb/DG5XWv4dg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKRI9BLx4jmAckS5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjS+nfntJ6oNU/LBThIaCjyULGYEWye1eoYNBe6XK37VnwOtkiAnFcjR6Je/egNFUkGlJRwb0w38xIYZ1pYRTqelXmpogskYD2nXUYkFNWE2v3aKzpwyQLHSrqRFc/X3RIaFMRMRuU6B7cgsezPxP6+b2vg6zJhMUkslWSyKU46sQrPX0YBpSiyfOIKJZu5WREZYY2JdQCUXQrD88ippXVSDWjW4v6zUb/I4inACp3AOAVxBHe6gAU0g8AjP8ApvnvJevHfvY9Fa8PKZY/gD7/MHnlePKQ==</latexit> ReLU<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="NDYzpDPf4NReujBVQrH7hehsVpg=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzU8Prlilt15yCrxMtJBXLU++Wv3iBmaYTSMEG17npuYvyMKsOZwGmpl2pMKBvTIXYtlTRC7WfzQ6fkzCoDEsbKljRkrv6eyGik9SQKbGdEzUgvezPxP6+bmvDGz7hMUoOSLRaFqSAmJrOvyYArZEZMLKFMcXsrYSOqKDM2m5INwVt+eZW0LqreVdVrXFZqt3kcRTiBUzgHD66hBvdQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBfJWMuw==</latexit>1<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
Context Scores<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d
<latexit sha1_base64="r3RhxPibzptum5p4iWVmnNh7d98=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzAOSJcxOZpMhM7PLPISw5Be8eFDEqz/kzb9xNtmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtnVhFaIskPFHdCGvKmaQtwwyn3VRRLCJOO9HkLvc7T1RplshHM01pKPBIspgRbHKpr60YVGt+3Z8DrZKgIDUo0BxUv/rDhFhBpSEca90L/NSEGVaGEU5nlb7VNMVkgke056jEguowm986Q2dOGaI4Ua6kQXP190SGhdZTEblOgc1YL3u5+J/Xsya+CTMmU2uoJItFseXIJCh/HA2ZosTwqSOYKOZuRWSMFSbGxVNxIQTLL6+S9kU9uKoHD5e1xm0RRxlO4BTOIYBraMA9NKEFBMbwDK/w5gnvxXv3PhatJa+YOYY/8D5/ADPjjlk=</latexit>X<latexit sha1_base64="8HotZ7L9jIC+JMZElREUEsd2+jQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUGPfLFbfqzkFWiZeTCuSo98tfvUHM0gilYYJq3fXcxPgZVYYzgdNSL9WYUDamQ+xaKmmE2s/mh07JmVUGJIyVLWnIXP09kdFI60kU2M6ImpFe9mbif143NeGNn3GZpAYlWywKU0FMTGZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kD1H2M9Q==</latexit>k
<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>dLinear<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="eA6cC+Yke2w6+xndKmHxhmczT1M=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cW7Ae0oWw2k3btZhN2N0Ip/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAqujet+O4W19Y3NreJ2aWd3b/+gfHjU0kmmGDZZIhLVCahGwSU2DTcCO6lCGgcC28Hobua3n1BpnsgHM07Rj+lA8ogzaqzUCPvlilt15yCrxMtJBXLU++WvXpiwLEZpmKBadz03Nf6EKsOZwGmpl2lMKRvRAXYtlTRG7U/mh07JmVVCEiXKljRkrv6emNBY63Ec2M6YmqFe9mbif143M9GNP+EyzQxKtlgUZYKYhMy+JiFXyIwYW0KZ4vZWwoZUUWZsNiUbgrf88ippXVS9q6rXuKzUbvM4inACp3AOHlxDDe6hDk1ggPAMr/DmPDovzrvzsWgtOPnMMfyB8/kDyeGM7g==</latexit>d<latexit sha1_base64="gMTgjs7J9T7tfl8I4J/4iGZ8J/U=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APbUjLpnTY0kxmSjFiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybnHjwXXxnW/ncLK6tr6RnGztLW9s7tX3j9o6ihRDBssEpFq+1Sj4BIbhhuB7VghDX2BLX98k/mtR1SaR/LeTGLshXQoecAZNVZ66IbUjPwgfZr2yxW36s5AlomXkwrkqPfLX91BxJIQpWGCat3x3Nj0UqoMZwKnpW6iMaZsTIfYsVTSEHUvnSWekhOrDEgQKfukITP190ZKQ60noW8ns4R60cvE/7xOYoKrXsplnBiUbP5RkAhiIpKdTwZcITNiYgllitushI2ooszYkkq2BG/x5GXSPKt6F9Xzu/NK7TqvowhHcAyn4MEl1OAW6tAABhKe4RXeHO28OO/Ox3y04OQ7h/AHzucPADaRJQ==</latexit>x
<latexit sha1_base64="Orsd8Uetr8OD0z/KWA5V57CS7gM=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae2Q8mkmTY0kwxJRhiG/oUbF4q49W/c+Tdm2llo64HA4Zx7ybkniDnTxnW/ndLa+sbmVnm7srO7t39QPTzqaJkoQttEcql6AdaUM0HbhhlOe7GiOAo47QbT29zvPlGlmRQPJo2pH+GxYCEj2FjpcRBhMwnCLJ0NqzW37s6BVolXkBoUaA2rX4ORJElEhSEca9333Nj4GVaGEU5nlUGiaYzJFI9p31KBI6r9bJ54hs6sMkKhVPYJg+bq740MR1qnUWAn84R62cvF/7x+YsJrP2MiTgwVZPFRmHBkJMrPRyOmKDE8tQQTxWxWRCZYYWJsSRVbgrd88irpXNS9y3rjvlFr3hR1lOEETuEcPLiCJtxBC9pAQMAzvMKbo50X5935WIyWnGLnGP7A+fwBAbuRJg==</latexit>y<latexit sha1_base64="b2P15Xg35B2Uc1NdmMfGE2pXebI=">AAACBXicbVC7SgNBFJ2NrxhfUUstFoNgFXYlqGXQRrCJYB6QXcLs5CYZMvtg5q4Ylm1s/BUbC0Vs/Qc7/8bJZgtNPDBwOOe+5niR4Aot69soLC2vrK4V10sbm1vbO+XdvZYKY8mgyUIRyo5HFQgeQBM5CuhEEqjvCWh746up374HqXgY3OEkAtenw4APOKOopV750EF4wGxOIqGfJo5PccSoSG7StFeuWFUrg7lI7JxUSI5Gr/zl9EMW+xAgE1Sprm1F6CZUImcC0pITK4goG9MhdDUNqA/KTbLtqXmslb45CKV+AZqZ+rsjob5SE9/TldMb1bw3Ff/zujEOLtyEB1GMELDZokEsTAzNaSRmn0tgKCaaUCa5vtVkIyopQx1cSYdgz395kbROq/ZZtXZbq9Qv8ziK5IAckRNik3NSJ9ekQZqEkUfyTF7Jm/FkvBjvxsestGDkPfvkD4zPHzrimbA=</latexit>K<latexit sha1_base64="Ta0kqL4N5jGMEBTHhbK9R3DfFLU=">AAACBXicbVDLSsNAFJ34rPVVdamLYBFclUSKuiy6cVnBPqAtZTK5aYdOJmHmRiwhGzf+ihsXirj1H9z5N07bLLT1wMDhnPua48WCa3Scb2tpeWV1bb2wUdzc2t7ZLe3tN3WUKAYNFolItT2qQXAJDeQooB0roKEnoOWNrid+6x6U5pG8w3EMvZAOJA84o2ikfumoi/CA0zmpAj9LuyHFIaMibWZZv1R2Ks4U9iJxc1ImOer90lfXj1gSgkQmqNYd14mxl1KFnAnIit1EQ0zZiA6gY6ikIeheOt2e2SdG8e0gUuZJtKfq746UhlqPQ89UTm7U895E/M/rJBhc9lIu4wRBstmiIBE2RvYkEtvnChiKsSGUKW5utdmQKsrQBFc0IbjzX14kzbOKe16p3lbLtas8jgI5JMfklLjkgtTIDamTBmHkkTyTV/JmPVkv1rv1MStdsvKeA/IH1ucPS6SZuw==</latexit>V<latexit sha1_base64="iLkvU3wtIpFtLSPmMsHyo5TUMdM=">AAACWnicbVBNS8NAEN3G7+/6cdNDsCgepCQi6lH0orcKVoW2lMlmqks32bA7kZaQi7/Gq/4bwR/jNs3BVgcWHm9m9s17QSKFIc/7qjgzs3PzC4tLyyura+sb1c2tB6NSzbHJlVT6KQCDUsTYJEESnxKNEAUSH4P+9aj/+IraCBXf0zDBTgTPsegJDmSpbnWvTTig4p9MY5hn7QjohYPMbvO8W615da8o9y/wS1BjZTW6m5XDdqh4GmFMXIIxLd9LqJOBJsEl5svt1GACvA/P2LIwhghNJyvkc/fAMqHbU9q+mNyC/b2RQWTMMArs5OhIM90bkf/1Win1LjqZiJOUMOZjoV4qXVLuKBM3FBo5yaEFwLWwt7r8BTRwsslNqARK9QkCM+Eks0wqQQ+Oi5MpkJNGw1eRmNLqYOzVJutP5/gXPJzU/bP66d1p7fKqzHiR7bJ9dsR8ds4u2Q1rsCbj7I29sw/2Wfl2HGfJWRmPOpVyZ5tNlLPzAxYxuNo=</latexit>I (c) Separable self-attention (ours)
Figure 3: Different self-attention units. (a) is a standard multi-headed self-attention (MHA) in
transformers. (b)extends MHA in (a) by introducing token projection layers, which project ktokens
to a pre-deﬁned number of tokens p, thus reducing the complexity from O(k2)toO(k). However,
it still uses costly operations (e.g., batch-wise matrix multiplication) for computing self-attention,
impacting latency on resource-constrained devices (Fig. 1). (c)is the proposed separable self-attention
layer that is linear in complexity, i.e., O(k), and uses element-wise operations for faster inference.
with two separate linear computations. The improved model, MobileViTv2 , is obtained by replacing
MHA with separable self-attention in MobileViT.
In the rest of this section, we ﬁrst brieﬂy describe MHA (Section 3.1), and then elaborate on the
details of separable self-attention (Section 3.2) and MobileViTv2 architecture (Section 3.3).
3.1 Overview of multi-headed self-attention
MHA (Fig. 3a) allows transformer to encode inter-token relationships. Speciﬁcally, MHA takes an
input x2Rkdcomprising of kd-dimensional token (or patch) embeddings. The input xis then fed
to three branches, namely query Q, keyK, and valueV. Each branch (Q,K, andV) is comprised of
hlinear layers (or heads), which enables the transformer to learn multiple views of the input. The
dot-product between the output of linear layers in QandKis then computed simultaneously for
allhheads, and is followed by a softmax operation to produce an attention (or context-mapping)
matrix a2Rkkh. Another dot-product is then computed between aand the output of linear layers
inVto produce weighted sum output yw2Rkdhh, wheredh=d
his the head dimension . The
outputs ofhheads are concatenated to produce a tensor with kd-dimensional tokens, which is then
fed to another linear layer with weights WO2Rddto produce the output of MHA y2Rkd.
Mathematically, this operation can be described as:
y=Concat0
BB@h 
hxWQ0;xWK0i
|{z}
a02Rkk;xWV0i;;h
hxWQh;xWKhi
|{z}
ah2Rkk;xWVhi1
CCAWO (1)
where WQi2Rddh,WKi2Rddh, and WVi2Rddhare the weights of the i-th linear layer
(or head) inQ,K, andVbranches respectively. The symbol h;idenotes the dot-product operation.
3.2 Separable self-attention
The structure of separable self-attention is inspired by MHA. Similar to MHA, the input xis processed
using three branches, i.e., input I, keyK, and valueV. The input branch Imaps eachd-dimensional
token in xto a scalar using a linear layer with weights WI2Rd. The weights WIserves as the
latent node Lin Fig. 4b. This linear projection is an inner-product operation and computes the
distance between latent token Landx, resulting in a k-dimensional vector. A softmax operation is
then applied to this k-dimensional vector to produce context scores cs2Rk. Unlike transformers
that compute the attention (or context) score for each token with respect to all ktokens, the proposed
4
--- Page 5 ---
ij<latexit sha1_base64="kKU4slhRI+NaZu6rgC+4JkqVhio=">AAACj3icbVDLbhMxFHWGVwmvBJZIyCJBYkM0gyrKCkWwobsikbYoE0XXnpvUjcce2ddto1G+je/gA9jCL+AkI0RarmTp6Jz78Dmi0spTmv5oJbdu37l7b+9++8HDR4+fdLpPj70NTuJIWm3dqQCPWhkckSKNp5VDKIXGE7H4tNZPLtB5Zc1XWlY4KWFu1ExJoEhNO99ygXNlagIRNLhVrVftnPCK6kNj0L2pnC2CJC6QLhHNiuc53+pkF2h4X/U5mIL3z/txEE3xd9O000sH6ab4TZA1oMeaOpp2Wy/ywspQoiGpwftxllY0qcGRkhrj+uCxArmAOY4jNFCin9SbDFb8VWQKPrMuPkN8w/47UUPp/bIUsbMEOvPXtTX5P20caPZ+UitTBUIjt4dmQXOyfB0oL5RDSXoZAUin4l+5PAMHkmLsO1eEtYuYjt9xUpdBk3L2ctdfcaEq3zi82lpsx0Sz6/ndBMdvB9m7wf6X/d7wY5PtHnvOXrLXLGMHbMg+syM2YpJ9Zz/ZL/Y76SYHyYdkuG1NWs3MM7ZTyeEf4B/Mbw==</latexit>Inner-product betweentokeniandjj<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤<latexit sha1_base64="ZX3R0tpnoWyEDov8q1UhyJUzgHM=">AAACSXicbVDLTgJBEJwFH4gvkKOJmUhMPBiya4h6JHrx4AETeSSAZHaYhZHZnc1ML0o2fItX/Ra/wM/wZjw5wB4E7KSTSnV3uqrcUHANtv1ppdJr6xubma3s9s7u3n4uf1DXMlKU1agUUjVdopngAasBB8GaoWLEdwVruMOb6bwxYkpzGTzAOGQdn/QD7nFKwFDdXKHtExi4Xky7evIYP53hu0k3V7RL9qzwKnASUERJVbt566jdkzTyWQBUEK1bjh1CJyYKOBVskm1HmoWEDkmftQwMiM90J56pn+ATw/SwJ5XpAPCM/XsRE1/rse+azalWvTybkv/NWhF4V52YB2EELKDzR14kMEg8jQL3uGIUxNgAQhU3WjEdEEUomMAWvrhSDoG4esFJ7EcCuJLPi/56Ix7qxOHL3GLWJOos57cK6ucl56JUvi8XK9dJthl0iI7RKXLQJaqgW1RFNUTRGL2iN/RufVhf1rf1M19NWclNAS1UKv0LmD2zWw==</latexit>csj,L<latexit sha1_base64="vJakunoZWF//T0puXfAbsX9ypv0=">AAACrHicbVFdb9MwFHXC1+gG6+ARCVm0CB5QlaAJ9jjBCw88DKndJjWlsp2bzqtjR/ZNt8rKD0Xix+C0EaIbV7J0dM+5X8e8UtJhkvyK4gcPHz1+sve0t3/w7Plh/+jFuTO1FTARRhl7yZkDJTVMUKKCy8oCK7mCC7782vIXK7BOGj3GdQWzki20LKRgGFLz/irjsJDaI+O1YrbxqullCLfox2YJmg6vh+8cXTFVg6PMAnWCKcgbmmV0q+NrKoxuYeBMUAyzkuEVL7yYu+anv/5AvzfD0BV0/nfMvD9IRskm6H2QdmBAujibH0Wvs9yIugSNQjHnpmlS4cwzi1IoCO1rBxUTS7aAaYCaleBmfmNQQ9+GTE4LY8PTSDfZfys8K51blzwo293dXa5N/o+b1liczLzUVY2gxXZQUSuKhrZu01xaEKjWATBhZdiViitmmcDwJztTuDHL4I7bucSXtUJpzc3ufflKVq678HZ7Yi84mt717z44/zhKP42OfxwPTr903u6RV+QNeU9S8pmckm/kjEyIIL+jONqPDuJRPI6n8WwrjaOu5iXZibj4A00H1UE=</latexit>Tokenj’s values are scaledby context scorecsj,L
12
31’2’
3’
<latexit sha1_base64="eFAanIKgMqnVzBBLNknN3NCMCYc=">AAACMHicbVBNSwMxFMzW7/pV9eglWEQPUnalqMeiF48KVoV2Ldn0bRua3SzJW7Use/DXeFV/jZ7Eqz9CTGsP1joQGGbeyyQTJFIYdN03pzA1PTM7N79QXFxaXlktra1fGpVqDnWupNLXATMgRQx1FCjhOtHAokDCVdA7GfhXt6CNUPEF9hPwI9aJRSg4Qyu1SptNhHsc3pMFMoU8YzeZt0e9nTxvlcpuxR2CThJvRMpkhLNW6avZVjyNIEYumTENz03Qz5hGwSXkxWZqIGG8xzrQsDRmERg/G4bndNsqbRoqbU+MdKj+3shYZEw/CuxkxLBr/noD8T+vkWJ45GciTlKEmP8EhamkqOigEdoWGjjKviWMa2HfSnmXacbR9jaWEijVQxaYsZ9kUSpRaHWXF21h3t96JsnlfsU7qFTPq+Xa8ai6ebJJtsgu8cghqZFTckbqhJMH8kieyLPz4rw6787Hz2jBGe1skDE4n99+A6r0</latexit>a1,10
<latexit sha1_base64="Z0OK05MF+4fBuGU0uLONeWJsBIs=">AAACMHicbVBNSwMxFMz6WetX1WMvwSJ6kLIroh6LXjxWsLbQriWbvtXQ7GZJ3qpl2YO/xqv6a/QkXv0RYlp7sOpAYJh5L5NMkEhh0HVfnanpmdm5+cJCcXFpeWW1tLZ+YVSqOTS4kkq3AmZAihgaKFBCK9HAokBCM+ifDP3mDWgjVHyOgwT8iF3FIhScoZW6pXIH4Q5H92SBTCHP2GXm7dK97Tzvlipu1R2B/iXemFTIGPVu6bPTUzyNIEYumTFtz03Qz5hGwSXkxU5qIGG8z66gbWnMIjB+NgrP6ZZVejRU2p4Y6Uj9uZGxyJhBFNjJiOG1+e0Nxf+8dorhkZ+JOEkRYv4dFKaSoqLDRmhPaOAoB5YwroV9K+XXTDOOtreJlECpPrLATPwki1KJQqvbvGgL837X85dc7FW9g+r+2X6ldjyurkDKZJPsEI8ckho5JXXSIJzckwfySJ6cZ+fFeXPev0ennPHOBpmA8/EFf7aq9Q==</latexit>a1,20
<latexit sha1_base64="jwm2tioidFKHgyz5b6qN1mfH4es=">AAACMHicbVA9T8MwFHT4pnwFGLtYVAgGVCVQASOChbFIlFZqS+W4r2DViSP7BaiiDPwaVuDXwIRY+REIN3SgwEmWTnfv+ewLYikMet6rMzE5NT0zOzdfWFhcWl5xV9cujEo0hxpXUulGwAxIEUENBUpoxBpYGEioB/2ToV+/AW2Eis5xEEM7ZFeR6AnO0Eodt9hCuMP8njSQCWQpu0z9Hbq3lWUdt+SVvRz0L/FHpERGqHbcz1ZX8SSECLlkxjR9L8Z2yjQKLiErtBIDMeN9dgVNSyMWgmmneXhGN63SpT2l7YmQ5urPjZSFxgzCwE6GDK/Nb28o/uc1E+wdtlMRxQlCxL+DeomkqOiwEdoVGjjKgSWMa2HfSvk104yj7W0sJVCqjywwYz9Jw0Si0Oo2K9jC/N/1/CUXu2V/v1w5q5SOjkfVzZEi2SDbxCcH5IickiqpEU7uyQN5JE/Os/PivDnv36MTzmhnnYzB+fgCgWmq9g==</latexit>a1,30
<latexit sha1_base64="GrEsGoBbYMapICRe/nCHmBlpaC4=">AAACUnicbVJNSwMxEM3W7/pV9ahIsIgepOyqqEfRgx4VrAptLdl0qqHZZElma8uyF3+NV/0tXvwrnkzbPVh1IPB4M5OX90gYS2HR9z+9wsTk1PTM7FxxfmFxabm0snprdWI4VLmW2tyHzIIUCqooUMJ9bIBFoYS7sHM+6N91wVih1Q32Y2hE7FGJtuAMHdUsbdQReji8J70wACpL2UO6v0cPdrKsWSr7FX9Y9C8IclAmeV01V7zNekvzJAKFXDJra4EfYyNlBgWXkBXriYWY8Q57hJqDikVgG+lQPqPbjmnRtjbuKKRD9udGyiJr+1HoJiOGT/Z3b0D+16sl2D5ppELFCYLiI6F2IilqOsiEtoQBjrLvAONGuLdS/sQM4+iSG1MJte4gC+2YkzRKJAqjn8f9tboitrnD3shi0SUa/M7vL7jdrwRHlcPrw/LpWZ7tLFknW2SXBOSYnJJLckWqhJMX8kreyLv34X0V3C8ZjRa8fGeNjFVh4RsXp7UU</latexit>a2,30
<latexit sha1_base64="xf+AJy+5AIod1nVSqDYDwlmNCXQ=">AAACUnicbVJNSwMxEM3Wr1q/Wj0qEiyiBym7RdSj6EGPClaFtpZsOtXQbLIks2pZ9uKv8aq/xYt/xZNpuwerDgQeb2by8h4JYyks+v6nV5ianpmdK86XFhaXllfKldVrqxPDocG11OY2ZBakUNBAgRJuYwMsCiXchP3TYf/mEYwVWl3hIIZ2xO6V6AnO0FGd8kYL4RlH96RnBkBlKbtL63s02MmyTrnq1/xR0b8gyEGV5HXRqXibra7mSQQKuWTWNgM/xnbKDAouISu1Egsx4312D00HFYvAttORfEa3HdOlPW3cUUhH7M+NlEXWDqLQTUYMH+zv3pD8r9dMsHfUToWKEwTFx0K9RFLUdJgJ7QoDHOXAAcaNcG+l/IEZxtElN6ESat1HFtoJJ2mUSBRGP0366z6K2OYOn8cWSy7R4Hd+f8F1vRYc1PYv96vHJ3m2RbJOtsguCcghOSbn5II0CCcv5JW8kXfvw/squF8yHi14+c4amajC4jcT/7US</latexit>a2,10<latexit sha1_base64="0KA703VpA+uRMIpPpMQDEl/4Qkg=">AAACUHicbVBNbxMxEJ1NKYS00ARuICGrEWoPKNqFinKM4MIxSORDSkLk9U5SK971yp5NE61W6q/ptf0tvfFPuIHzgUQSRrL89GbGz++FqZKWfP+nVzp4dPj4Sflp5ej42fOTau1Fx+rMCGwLrbTphdyikgm2SZLCXmqQx6HCbjj9sux3Z2is1Ml3WqQ4jPkkkWMpODlqVH01IJzT6p3cYFTk/Ef+4R0LzopiVK37DX9VbB8EG1CHTbVGNe/NINIiizEhobi1/cBPaZhzQ1IoLCqDzGLKxZRPsO9gwmO0w3wlXrC3jonYWBt3EmIr9t+NnMfWLuLQTcacruxub0n+r9fPaPxpmMskzQgTsRYaZ4qRZstEWCQNClILB7gw0v2ViStuuCCX25ZKqPWUeGi3nORxpkgafb3tL5rJ1G4cztcWKy7RYDe/fdB53wg+Ni6+XdSbnzfZluE1nMI5BHAJTfgKLWiDgBu4hTu49x68X97vkrce/XvDS9iqUuUPjqK1SA==</latexit>a3,10
<latexit sha1_base64="8Oh3R0cyhhcFCyperBwDNduZXUA=">AAACUHicbVBNSyNBEK2JumpWd6PeFKTZIHqQMJMV9Sh68ahgVEhi6OmpaJOe6aG7Rg3DgL9mr+tv8eY/8aadD8GoBU0/XlX16/fCVElLvv/slaamZ37Mzs2Xfy4s/vpdWVo+tzozAhtCK20uQ25RyQQbJEnhZWqQx6HCi7B3NOhf3KKxUidn1E+xHfPrRHal4OSoTmW1RXhPw3dyg1GR86v87zarbxZFp1L1a/6w2FcQjEEVxnXSWfLWW5EWWYwJCcWtbQZ+Su2cG5JCYVFuZRZTLnr8GpsOJjxG286H4gXbcEzEutq4kxAbsh83ch5b249DNxlzurGfewPyu14zo+5+O5dJmhEmYiTUzRQjzQaJsEgaFKT6DnBhpPsrEzfccEEutwmVUOse8dBOOMnjTJE0+m7SX3QrUzt2eD+yWHaJBp/z+wrO67Vgt7ZzulM9OBxnOwdr8Ae2IIA9OIBjOIEGCHiAf/AfHr0n78V7LXmj0fcbVmCiSuU3kHa1SQ==</latexit>a3,20
<latexit sha1_base64="nBDHjvohuoWIGe/u/kWtq7VsF3U=">AAACUHicbVBNTxsxEJ0NLYRQIMANpMoiqtoDinZJBBwjeukRpAaQkjTyeidgxbte2bOBaLUSv4Yr/BZu/Se9gfNRqYGOZPnpzYyf3wtTJS35/m+vtPTh4/JKebWy9ml9Y7O6tX1hdWYEtoVW2lyF3KKSCbZJksKr1CCPQ4WX4fD7pH85QmOlTn7SOMVezK8TOZCCk6P61d0u4R1N38kNRkXOf+WNA9b4WhT9as2v+9Ni70EwBzWY11l/y/vcjbTIYkxIKG5tJ/BT6uXckBQKi0o3s5hyMeTX2HEw4THaXj4VL9gXx0RsoI07CbEp++9GzmNrx3HoJmNON/Ztb0L+r9fJaHDSy2WSZoSJmAkNMsVIs0kiLJIGBamxA1wY6f7KxA03XJDLbUEl1HpIPLQLTvI4UySNvl30F41kaucO72YWKy7R4G1+78HFYT04qjfPm7XW6TzbMuzBPnyDAI6hBT/gDNog4B4e4BGevGfvj/dS8majf2/YgYUqVV4Bkkq1Sg==</latexit>a3,30
<latexit sha1_base64="oP6iIk5zs1r/eDQM975boGseCSI=">AAACUnicbVJNSwMxEM3W7/pV9ahIsIgepOwWUY+iBz1WsFVoa8mmUw3NJksyq5ZlL/4ar/pbvPhXPJm2e7DqQODxZiYv75EwlsKi7396hanpmdm5+YXi4tLyymppbb1hdWI41LmW2tyGzIIUCuooUMJtbIBFoYSbsH8+7N88grFCq2scxNCO2L0SPcEZOqpT2mohPOPonvTCAKgsZXdp9YBW97KsUyr7FX9U9C8IclAmedU6a952q6t5EoFCLpm1zcCPsZ0yg4JLyIqtxELMeJ/dQ9NBxSKw7XQkn9Fdx3RpTxt3FNIR+3MjZZG1gyh0kxHDB/u7NyT/6zUT7J20U6HiBEHxsVAvkRQ1HWZCu8IARzlwgHEj3Fspf2CGcXTJTaiEWveRhXbCSRolEoXRT5P+uo8itrnD57HFoks0+J3fX9CoVoKjyuHVYfn0LM92nmySHbJPAnJMTsklqZE64eSFvJI38u59eF8F90vGowUv39kgE1VY+gYV07UT</latexit>a2,20Query tokensKey tokens
<latexit sha1_base64="jrC/j5Xv7brc+Hbkkc8yQ09POBg=">AAACYXicbVBNSwMxEE3X7/rV6rEgwSIIQtmVoh5FLx4VbBXaUmbTrIYmmyWZrS1Lf4S/xqv+DM/+EbPtHqw6EHi8N5OZ98JECou+/1nylpZXVtfWN8qbW9s7u5XqXtvq1DDeYlpq8xiC5VLEvIUCJX9MDAcVSv4QDq9z/WHEjRU6vsdJwnsKnmIRCQboqH7lpIt8jNm1VkmKnAIij3OFKkAjxnRKuw49h1EG036l7jf8WdG/IChAnRR126+WDroDzVLl/mQSrO0EfoK9DAwKJvm03E0tT4AN4Yl3HIxBcdvLZq6m9MgxAxpp416MdMb+nMhAWTtRoevMT7S/tZz8T+ukGF30MhHnjmM2XxSlkqKmeUR0IAxnKCcOADPC3UrZMxhg6IJc2BJqPUQI7YKTTKUShdEvi/4GI5HYwuF4brHsEg1+5/cXtE8bwVmjedesX14V2a6TGjkkxyQg5+SS3JBb0iKMvJI38k4+Sl/ehlfx9uatXqmY2ScL5dW+AXu1uwM=</latexit>Compute attention matrixa(a) Self-attention in transformers
123Input tokens
L
<latexit sha1_base64="8q1Ps2eM7n8OC6rhUlk92JoEfvA=">AAACYXicbVBNS8NAEN3Gr1q/aj0WZLEIglASKeqx2IvHCrYKTSmb7UaX7mbD7qS2hP4If41X/Rme/SNu2hyMOqc3b2aY914QC27AdT9Lztr6xuZWebuys7u3f1A9rPWNSjRlPaqE0o8BMUzwiPWAg2CPsWZEBoI9BJNONn+YMm24iu5hHrOhJE8RDzklYKlR9dwHNoO0o2ScAMNURVmPp4yC0niBfUngOQhTOpouRtWG23SXhf8CLwcNlFd3dFg69seKJpJFQAUxZuC5MQxTooFTwRYVPzEsJnRCntjAwohIZobp0tUCn1pmjEMrI7Sq8JL9eZESacxcBnYzE2l+zzLyv9kggfB6mPIocxzR1aMwERgUziLCY66tfTG3gFDNrVZMn4kmFGyQhS+BUhMggSk4SWUigGv1UvQ3nvLY5A5nK4sVm6j3O7+/oH/R9C6brbtWo32TZ1tGdXSCzpCHrlAb3aIu6iGKXtEbekcfpS9n26k6tdWqU8pvjlChnPo3gbC7Bw==</latexit>Compute context vectorcv
<latexit sha1_base64="aHfWEKiAVGS+vJisi00U66pH9CI=">AAACW3icbVBNSysxFE3H7/pVFVeCBIvgQsqMFN9bynPjwoWCVaGtJUnvaGhmMiR3fJYwv8Bf41Z/iQv/i2k7C6seCBzOvTf3nsMzJS2G4XslmJmdm19YXKour6yurdc2Nq+tzo2AltBKm1vOLCiZQgslKrjNDLCEK7jhg9NR/eYRjJU6vcJhBt2E3acyloKhl3q1/Q7CE47/cVzlULhOwvCBx070bHHnokN6XhS9Wj1shGPQnyQqSZ2UuOhtVHY7fS3yBFIUilnbjsIMu44ZlEJBUe3kFjImBuwe2p6mLAHbdeM7CrrvlT6NtfEvRTpWv044llg7TLjvHB1rv9dG4m+1do7x366TaZYjpGKyKM4VRU1H4dC+NCBQDT1hwkh/KxUPzDCBPsKpLVzrATJup5y4JFcojf4/7a//KDNbOnyaWKz6RKPv+f0k10eN6LjRvGzWT/6V2S6SHbJHDkhE/pATckYuSIsI8kxeyCt5q3wEM0E1WJm0BpVyZotMIdj+BH7YuLE=</latexit>cs1,L
<latexit sha1_base64="t9PN/CnU0tDvdtgnKc967oTZzGo=">AAACXHicbVBNSyNBEO2M3/ErruBlQRqD4kHCjIjuUfSwHjy4YFRIYujp1JgmPd1Dd40amvkH/hqvu39kL/4WO8kcjPqg4fGqqqveizMpLIbh/0owMzs3v7C4VF1eWV1br238uLE6NxyaXEtt7mJmQQoFTRQo4S4zwNJYwm08OB/Vbx/BWKHVNQ4z6KTsQYlEcIZe6tb22gjPOP7H/TYAqnDtlGE/Thzv2uLeHR7Qy6Lo1uphIxyDfiVRSeqkxFV3o7Ld7mmep6CQS2ZtKwoz7DhmUHAJRbWdW8gYH7AHaHmqWAq248aHFHTXKz2aaOOfQjpWP044llo7TGPfOTrWfq6NxO9qrRyTXx0nVJYjKD5ZlOSSoqajdGhPGOAoh54wboS/lfI+M4yjz3BqS6z1AFlsp5y4NJcojH6a9td7FJktHT5PLFZ9otHn/L6Sm8NGdNw4+nNUPz0rs10kP8kO2ScROSGn5IJckSbh5IW8kr/kX+UtmA2Wg9VJa1ApZzbJFIKtdzcduQU=</latexit>cs2,L1’2’3’Key tokens
<latexit sha1_base64="o+44f8/mvQjaFw1rq4ED37OSRt0=">AAACQHicbVDLSsNAFJ34rPXV6lKQYBFclUSKuiy6cVnBPqAJZTKZtEMnmTBzUy2hv+FWv8W/8A/ciVtXTtosTOuFC4dz7+Wec7yYMwWW9WGsrW9sbm2Xdsq7e/sHh5XqUUeJRBLaJoIL2fOwopxFtA0MOO3FkuLQ47Trje+yeXdCpWIieoRpTN0QDyMWMIJBU44TYhh5QUoGk9mgUrPq1rzMVWDnoIbyag2qxqnjC5KENALCsVJ924rBTbEERjidlZ1E0RiTMR7SvoYRDqly07nomXmuGd8MhNQdgTln/16kOFRqGnp6MxOplmcZ+d+sn0Bw46YsihOgEVk8ChJugjCzBEyfSUqATzXARDKt1SQjLDEBnVPhiyfEGLCnCk7SMOHApHgq+vMnLFa5w+eFxbJO1F7ObxV0Luv2Vb3x0Kg1b/NsS+gEnaELZKNr1ET3qIXaiKAYvaBX9Ga8G5/Gl/G9WF0z8ptjVCjj5xersLGO</latexit>cv<latexit sha1_base64="aHfWEKiAVGS+vJisi00U66pH9CI=">AAACW3icbVBNSysxFE3H7/pVFVeCBIvgQsqMFN9bynPjwoWCVaGtJUnvaGhmMiR3fJYwv8Bf41Z/iQv/i2k7C6seCBzOvTf3nsMzJS2G4XslmJmdm19YXKour6yurdc2Nq+tzo2AltBKm1vOLCiZQgslKrjNDLCEK7jhg9NR/eYRjJU6vcJhBt2E3acyloKhl3q1/Q7CE47/cVzlULhOwvCBx070bHHnokN6XhS9Wj1shGPQnyQqSZ2UuOhtVHY7fS3yBFIUilnbjsIMu44ZlEJBUe3kFjImBuwe2p6mLAHbdeM7CrrvlT6NtfEvRTpWv044llg7TLjvHB1rv9dG4m+1do7x366TaZYjpGKyKM4VRU1H4dC+NCBQDT1hwkh/KxUPzDCBPsKpLVzrATJup5y4JFcojf4/7a//KDNbOnyaWKz6RKPv+f0k10eN6LjRvGzWT/6V2S6SHbJHDkhE/pATckYuSIsI8kxeyCt5q3wEM0E1WJm0BpVyZotMIdj+BH7YuLE=</latexit>cs1,L
<latexit sha1_base64="t9PN/CnU0tDvdtgnKc967oTZzGo=">AAACXHicbVBNSyNBEO2M3/ErruBlQRqD4kHCjIjuUfSwHjy4YFRIYujp1JgmPd1Dd40amvkH/hqvu39kL/4WO8kcjPqg4fGqqqveizMpLIbh/0owMzs3v7C4VF1eWV1br238uLE6NxyaXEtt7mJmQQoFTRQo4S4zwNJYwm08OB/Vbx/BWKHVNQ4z6KTsQYlEcIZe6tb22gjPOP7H/TYAqnDtlGE/Thzv2uLeHR7Qy6Lo1uphIxyDfiVRSeqkxFV3o7Ld7mmep6CQS2ZtKwoz7DhmUHAJRbWdW8gYH7AHaHmqWAq248aHFHTXKz2aaOOfQjpWP044llo7TGPfOTrWfq6NxO9qrRyTXx0nVJYjKD5ZlOSSoqajdGhPGOAoh54wboS/lfI+M4yjz3BqS6z1AFlsp5y4NJcojH6a9td7FJktHT5PLFZ9otHn/L6Sm8NGdNw4+nNUPz0rs10kP8kO2ScROSGn5IJckSbh5IW8kr/kX+UtmA2Wg9VJa1ApZzbJFIKtdzcduQU=</latexit>cs2,L
<latexit sha1_base64="O0FUQn+gceBp1dpR/cJ8h+IUh2o=">AAACWnicbVBNTxsxEHWWUvJBS6DckCqLtFIPVbTbIuAY0QsHDkEiJFKSRl7vLFjxrlf2LBBZ+wf4NVzbf1KpPwYn2QMJPMnS05sZz7wXZlIY9P1/FW/j3eb7rWqt3tj+8HGnubt3bVSuOfS4kkoPQmZAihR6KFDCINPAklBCP5z+mtf7d6CNUOkVzjIYJ+wmFbHgDJ00aX4ZITzg4h+rISrsKGF4G8aWT0zx2/78Ti+KYtJs+W1/AfqaBCVpkRLdyW7l8yhSPE8gRS6ZMcPAz3BsmUbBJRT1UW4gY3zKbmDoaMoSMGO7OKOgX50S0Vhp91KkC/XlhGWJMbMkdJ3zY816bS6+VRvmGJ+OrUizHCHly0VxLikqOs+GRkIDRzlzhHEt3K2U3zLNOLoEV7aESk2RhWbFiU1yiUKr+1V/0Z3ITOnwYWmx7hIN1vN7Ta5/tIPj9tHlUatzVmZbJQfkkHwjATkhHXJOuqRHOHkkT+QP+Vv573lezWssW71KOfOJrMDbfwaHn7g8</latexit>cs3,L
<latexit sha1_base64="O0FUQn+gceBp1dpR/cJ8h+IUh2o=">AAACWnicbVBNTxsxEHWWUvJBS6DckCqLtFIPVbTbIuAY0QsHDkEiJFKSRl7vLFjxrlf2LBBZ+wf4NVzbf1KpPwYn2QMJPMnS05sZz7wXZlIY9P1/FW/j3eb7rWqt3tj+8HGnubt3bVSuOfS4kkoPQmZAihR6KFDCINPAklBCP5z+mtf7d6CNUOkVzjIYJ+wmFbHgDJ00aX4ZITzg4h+rISrsKGF4G8aWT0zx2/78Ti+KYtJs+W1/AfqaBCVpkRLdyW7l8yhSPE8gRS6ZMcPAz3BsmUbBJRT1UW4gY3zKbmDoaMoSMGO7OKOgX50S0Vhp91KkC/XlhGWJMbMkdJ3zY816bS6+VRvmGJ+OrUizHCHly0VxLikqOs+GRkIDRzlzhHEt3K2U3zLNOLoEV7aESk2RhWbFiU1yiUKr+1V/0Z3ITOnwYWmx7hIN1vN7Ta5/tIPj9tHlUatzVmZbJQfkkHwjATkhHXJOuqRHOHkkT+QP+Vv573lezWssW71KOfOJrMDbfwaHn7g8</latexit>cs3,L
<latexit sha1_base64="3A/mVRfJCaZ9GpsUYMykQS+za3M=">AAACYXicbVDLSgNBEJys7/iK8SjIYBAEIeyKqEcxF48KRoUkhNlJrw6ZxzLTq4YlH+HXeNXP8OyPOJvswagFAzXV3XRXxakUDsPwsxLMzS8sLi2vVFfX1jc2a1v1W2cyy6HNjTT2PmYOpNDQRoES7lMLTMUS7uJhq6jfPYF1wugbHKXQU+xBi0Rwhl7q1w67CC+Yt4xKMwTKjS7+1HFjwdEx7SqGj3GS874b92uNsBlOQP+SqCQNUuKqv1XZ7Q4MzxRo5JI514nCFHs5syi4hHG1mzlIGR+yB+h4qpkC18snrsZ03ysDmhjrn0Y6UX9O5Ew5N1Kx7yyOdL9rhfhfrZNhctbLhS4caz5dlGSSoqFFRHQgLHCUI08Yt8LfSvkjs4yjD3JmS2zMEFnsZpzkKpMorHme9Td4EqkrHb5MLVZ9otHv/P6S26NmdNI8vj5unF+U2S6THbJHDkhETsk5uSRXpE04eSVv5J18VL6ClaAW1KetQaWc2SYzCHa+AXS5uwA=</latexit>Compute context scorescs<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤
<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤
<latexit sha1_base64="zRcTmo6/KyQ8ShvUKIHWxZASHAU=">AAACNXicbVDLSsNAFJ34rPXV6lKQYBHERUmkqMuiG5ct2Ae0oUwm03boZCbM3FRL6Be41W/xW1y4E7f+gtM2C9N64MLh3Hu59xw/4kyD43xYa+sbm1vbuZ387t7+wWGheNTUMlaENojkUrV9rClngjaAAaftSFEc+py2/NH9rN8aU6WZFI8wiagX4oFgfUYwGKl+2SuUnLIzh71K3JSUUIpar2iddgNJ4pAKIBxr3XGdCLwEK2CE02m+G2saYTLCA9oxVOCQai+Zfzq1z40S2H2pTAmw5+rfjQSHWk9C30yGGIZ6uTcT/+t1YujfegkTUQxUkMWhfsxtkPbMth0wRQnwiSGYKGZ+tckQK0zAhJO54ks5AuzrjJMkjDkwJZ+y/oIxi3Tq8HlhMW8SdZfzWyXNq7J7Xa7UK6XqXZptDp2gM3SBXHSDqugB1VADEUTRC3pFb9a79Wl9Wd+L0TUr3TlGGVg/vzHTrEw=</latexit>⇤<latexit sha1_base64="+jqneO7rnvdYuFH6gonTHQiKhfE=">AAACOHicbVDLSsNAFJ34rPXV6lKQYBFclUSKuiy6cVnBPqANZTKZtkNnMmHmTrWE/oJb/Rb/xJ07cesXmLRZmNYDFw7n3su95/gRZxoc58NaW9/Y3Nou7BR39/YPDkvlo5aWRhHaJJJL1fGxppyFtAkMOO1EimLhc9r2x3dpvz2hSjMZPsI0op7Aw5ANGMGQSj1tRL9UcarOHPYqcTNSQRka/bJ12gskMYKGQDjWuus6EXgxVsAIp7Niz2gaYTLGQ9pNaIgF1V48f3ZmnydKYA+kSioEe67+3Yix0Hoq/GRSYBjp5V4q/tfrGhjceDELIwM0JItDA8NtkHbq3A6YogT4NCGYKJb8apMRVphAkk/uii/lGLCvc05iYTgwJZ/y/oIJi3Tm8HlhsZgk6i7nt0pal1X3qlp7qFXqt1m2BXSCztAFctE1qqN71EBNRNAIvaBX9Ga9W5/Wl/W9GF2zsp1jlIP18wtqP63x</latexit>X (b) Proposed separable self-attention method
Figure 4: Example illustrating the interaction between tokens to learn global representations
in different attention layers. In (a), each query token computes the distance with all key tokens via
dot-product. These distances are then normalized using softmax to produce an attention matrix a,
which encodes contextual relationships. In (b), the inner product between input tokens and latent
tokenLis computed. The resultant vector is normalized using softmax to produce context scores cs.
These context scores are used to weight key tokens and produce a context vector cv, which encodes
contextual information.
method only computes the context score with respect to a latent token L. This reduces the cost of
computing attention (or context) scores from O(k2)toO(k).
The context scores csare used to compute a context vector cv. Speciﬁcally, the input xis linearly
projected to a d-dimensional space using key branch Kwith weights WK2Rddto produce an
output xK2Rkd. The context vector cv2Rdis then computed as a weighted sum of xKas:
cv=kX
i=1cs(i)xK(i) (2)
The context vector cvis analogous to the attention matrix ain Eq. (1) in a sense that it also encodes
the information from all tokens in the input x, but is cheap to compute.
The contextual information encoded in cvis shared with all tokens in x. To do so, the input x
is linearly projected to a d-dimensional space using a value branch Vwith weights WV2Rdd,
followed by a ReLU activation to produce an output xV2Rkd. The contextual information in cv
is then propagated to xVvia broadcasted element-wise multiplication operation. The resultant output
is then fed to another linear layer with weights WO2Rddto produce the ﬁnal output y2Rkd.
Mathematically, separable self-attention can be deﬁned as:
y=0
BBBBBB@X0
B@cs2Rk
z}|{
(xWI)xWK1
CA
|{z}
cv2RdReLU (xWV)1
CCCCCCAWO (3)
whereandPare broadcastable element-wise multiplication and summation operations, respectively.
Comparison with self-attention methods Fig. 1 compares the proposed method with Transformer
and Linformer. Because time complexity of self-attention methods do not account for the cost of
operations that are used to implement these methods, some of the operations may become bottleneck
on resource-constrained devices. For holistic understanding, module-level latency on a single CPU
core with varying kis also measured in addition to theoretical metrics. The proposed separable
self-attention is fast and efﬁcient as compared to MHA in Transformer and Linformer.
Besides these module-level results, when we replaced the MHA in the transformer with the proposed
self-separable attention in the MobileViT architecture, we observe 3improvement in inference
5
--- Page 6 ---
Table 1: Effect of different self-attention methods on the performance of MobileViT [ 4] on the
ImageNet-1k dataset. Here, all models have similar number of parameters and FLOPs, and latency is
measured on iPhone12.
Attention unit Latency ##Top-1""
Self-attention in Transformer (Fig. 3a; [5]) 9.9 ms 78.4
Self-attention in Linformer (Fig. 3b; [10]) 10.2 ms 78.2
Separable self-attention (Ours; Fig. 3c) 3.4ms 78.1
speed with similar performance on the ImageNet-1k dataset (Table 1). These results show the
efﬁcacy of the proposed separable self-attention at the architecture-level. Note that self-attention
in Transformer and Linformer yields similar results for MobileViT. This is because the number of
tokenskin MobileViT is fewer ( k1024 ) as compared to language models, where Linformer is
signiﬁcantly faster than the transformer.
Relationship with additive addition The proposed approach resembles the attention mechanism
of Bahdanau et al. [26], which also encodes the global information by taking a weighted-sum of
LSTM outputs at each time step. Unlike [ 26], where input tokens interact via recurrence, the input
tokens in the proposed method interact only with a latent token.
3.3 MobileViTv2 architecture
To demonstrate the effectiveness of the proposed separable self-attention on resource-constrained
devices, we integrate separable self-attention with a recent ViT-based model, MobileViT [ 4]. Mobile-
ViT is a light-weight, mobile-friendly hybrid network that delivers signiﬁcantly better performance
than other competitive CNN-based, transformer-based, or hybrid models, including MobileNets
[27, 24, 25]. To avoid ambiguity, we refer to MobileViT as MobileViTv1 in the rest of the paper.
Speciﬁcally, we replace MHA in the transformer block in the MobileViTv1 with the proposed
separable self-attention method. We call the resultant architecture MobileViTv2 . We also do not
use the skip-connection and fusion block in the MobileViT block (Fig. 1b in [ 4]) as it improves the
performance marginally (Fig. 12 in [ 4]). Furthermore, to create MobileViTv2 models at different
complexities, we uniformly scale the width of MobileViTv2 network using a width multiplier
2f0:5;2:0g. This is in contrast to MobileViTv1 which trains three speciﬁc architectures (XXS, XS,
and S) for mobile devices. More details about MobileViTv2 ’s architecture are given in Appendix A.
4 Experimental results
4.1 Object classiﬁcation on the ImageNet dataset
Training on ImageNet-1k from scratch We train MobileViTv2 for 300 epochs with an effective
batch size of 1024 images (128 images per GPU 8 GPUs) using AdamW [ 28] on the ImageNet-1k
dataset [ 29] with 1.28 million and 50 thousand training and validation images respectively. We
linearly increase the learning rate from 10 6to0:002for the ﬁrst 20k iterations. After that, the
learning rate is decayed using a cosine annealing policy [ 30]. To reduce stochastic noise during
training, we use exponential moving average (EMA) [ 31] as we ﬁnd it helps larger models. We
implement our models using CVNets [4,32], and use their provided scripts for data processing,
training, and evaluation.
Pre-training on ImageNet-21k-P and ﬁnetuning on ImageNet-1k We train on the ImageNet-
21k (winter’21 release) that contains about 13 million images across 19k classes. Speciﬁcally, we
follow [ 33] to pre-process (e.g., remove classes with fewer samples) the dataset and split it into
about 11 million and 522 thousand training and validation images spanning over 10,450 classes,
respectively. Following [ 33], we refer to this pre-processed dataset as ImageNet-21k-P. Note that the
ImageNet-21k-P validation set does not overlap with the validation and test sets of ImageNet-1k.
We follow [ 33] for pre-training MobileViTv2 on ImageNet-21k-P. For faster convergence, we
initialize MobileViTv2 models with ImageNet-1k weights and ﬁnetune it on ImageNet-21k-P for 80
6
--- Page 7 ---
Table 2: Classiﬁcation performance on the ImageNet-1k validation set . Here, NS means that we
are not able to measure the latency on mobile device as some operations (e.g., cyclic shifts) are not
supported on mobile devices. Following [ 4], latency is measured on iPhone12 with a batch size of 1.
Similar to [ 3,34], throughput is measured on NVIDIA V100 GPUs with a batch size of 128. The
rows are grouped by network parameters.
Row # Model TypeNeural Extra Image# Params##FLOPs##Latency##Throughput""Top-1""
search? data size (in ms) (images/ sec) (in %)
R1 MobileViT-XXS [4] Hybrid 7 None 25621.3 M 0.4 G 4.8 4225 69.0
R2 MobileViTv2 -0.5 Hybrid 7 None 25621.4 M 0.5 G 1.6 4595 70.2
R3 MobileFormer-52 [35] Hybrid 7 None 22423.6 M 52 M 7.1 4445 68.7
R4 MobileViTv2 -1.0 Hybrid 7 None 25624.9 M 1.8 G 3.4 2351 78.1
R5 EfﬁcientNet-b0 [36] CNN 3 None 22425.3 M 422 M 1.6 4619 77.1
R6 DeiT-Tiny [2] Transformer 7 None 22425.5 M 1.3 G 3.4 4541 72.2
R7 MobileViT-S [4] Hybrid 7 None 25625.6 M 2.0 G 3.4 1986 78.4
R8 EfﬁcientNet-b2 [36] CNN 3 None 28829.1 M 1.2 G 3.8 2032 80.1
R9 MobileViTv2 -1.5 Hybrid 7 None 256210.6 M 4.0 G 5.1 1418 80.4
R10 MobileFormer-294 [35] Hybrid 7 None 224211.8 M 294 M 40.7 1402 77.9
R11 MobileViTv2 -2.0 Hybrid 7 None 256218.5 M 7.5 G 7.5 1105 81.2
R12 Swin-T [3] Hybrid 7 None 224228.3 M 4.5 G NS 1390 81.3
R13 ConvNext-T [34] CNN 7 None 224228.6 M 4.5 G 3.7 1800 82.1
R14 DeiT-Base [2] Transformer 7 None 224286.6 M 17.6 G 13.2 958 81.8
R15 MobileViTv2 -2.0 Hybrid 7 ImageNet-21k-P 256218.5 M 7.5 G 7.5 1105 82.4
R16 ConvNext-T [34] CNN 7 ImageNet-21k 224228.6 M 4.5 G 3.7 1800 82.9
R17 MobileViTv2 -2.0 Hybrid 7 ImageNet-21k-P 384218.5 M 16.1 G 17.0 488 83.4
R18 ConvNext-T [34] CNN 7 ImageNet-21k 384228.6 M 13.1 G 8.6 645 84.1
epochs with an effective batch size of 4096 images (128 images per GPU x 32 GPUs). We do not use
any linear warm-up. Other settings follow ImageNet-1k training.
We ﬁnetune ImageNet-21k-P pre-trained models on ImageNet-1k for 50 epochs using SGD with
momentum (0.9) and cosine annealing policy with an effective batch size of 256 images (128 images
per GPU2 GPUs).
Finetuning at higher resolution MobileViTv2 is a hybrid architecture that combines convolution
and separable self-attention to learn visual representations. Unlike many ViT-based models (e.g.,
DeiT), MobileViTv2 does not require adjustment to patch embeddings or positional biases for
different input resolutions and is simple to ﬁnetune. We ﬁnetune MobileViTv2 models at higher
resolution (i.e., 384384) for 10 epochs with a ﬁxed learning rate of 10 3using SGD.
Comparison with existing methods Table 2 and Fig. 2 compares MobileViTv2 ’s performance
with recent methods1. We make following observations:
•When MHA in MobileViTv1 is replaced with separable self-attention, the resultant model,
MobileViTv2 , is faster and better (Fig. 2); validating the effectiveness of the proposed sepa-
rable self-attention method for mobile ViTs.
•Compared to transformer-based (including hybrid) models, MobileViTv2 models are fast on
mobile devices. For example, MobileViTv2 is about 8faster on a mobile device and delivers 2.5%
better performance on the ImageNet-1k dataset than MobileFormer [ 35], even though MobileFormer
is FLOP efﬁcient (R9 vs. R10). However, on GPU, both MobileFormer and MobileViTv2 run at a
similar speed. The discrepancy in FLOPs and speed of MobileFormer across devices is primarily
because of its architectural design. MobileFormer has conditional operations between mobile and
former blocks. Such conditional operations, especially on resource-constrained devices, have a low
degree of parallelism and create memory bottlenecks, resulting in a high latency network. Ma et al.
[37] also makes a similar observation for CNN-based architectures.
•MobileViTv2 bridges the latency gap between CNN- and ViT-based models on mobile devices
while maintaining performance with similar or fewer parameters. For example, on a mobile device,
ConvNexT [ 34] (CNN-based model) is 2and3:6faster than MobileViTv2 (hybrid model)
and DeiT (transformer-based model) for similar performance respectively (see R11, R13, and
R14). The low latency of fully CNN-based models on mobile devices can be attributed to several
device-level optimizations that have been done for CNN-based models over the past few years
(e.g., dedicated hardware implementations for convolutions and folding batch normalization with
1For additional results including ablations, see Appendix B, Appendix C, and Appendix E.
7
--- Page 8 ---
Table 3: Semantic segmentation results on the ADE20k and the PASCAL VOC 2012 datasets.
Here, throughput, network parameters, and FLOPs are measured on the ADE20k dataset for an input
with a spatial resolution of 512512. mIoU (mean intersection over union) score is calculated for
a single scale only. Throughput is calculated using a batch size of 32 images on a single NVIDIA
V100 GPU with 32 GB memory and is an average of over 50 iterations (excluding 10 iterations for
warmup). We do not report latency on a mobile device as some of the operations (e.g., pyramid
pooling in PSPNet) are not optimally implemented for mobile devices. The baseline results are from
the MMSegmentation library [45]. Rows are grouped by network parameters.
Seg. ImageNet-1k Image Throughput "" # Params## FLOPs## mIoU""
Model Backbone Size (images/sec) (in millions) (in billions) ADE20k [42] PASCAL VOC [43]
PSPNet [40]MobileViTv2 -0.5 (Ours) 5122439 3.6 M 15.4 G 31.8 74.6
MobileNetv2 [24] 5122276 13.7 M 53.1 G 29.7 –
PSPNet [40]MobileViTv2 -1.75 (Ours) 5122114 22.5 M 95.9 G 39.8 80.2
ResNet-50 [44] 5122119 49.1 M 179.1 G 41.1 76.8
DeepLabv3 [41]MobileViTv2 -0.75 (Ours) 5122241 9.6 M 40.0 G 34.7 75.1 (=0.5)
MobileNetv2 [24] 5122246 18.7 M 75.4 G 34.1 –
DeepLabv3 [41]MobileViTv2 -2.0 (Ours) 512290 34.0 M 147.0 G 40.9 80.3 (=1.5)
ResNet-50 [44] 5122103 68.2 M 270.3 G 42.4 79.1
convolutions). ViT-based models still lack such optimizations and therefore, the resultant inference
graphs are sub-optimal. Though MobileViTv2 bridges the latency gap between CNNs and ViTs,
we believe the latency of ViT-based models will improve in the future with similar optimizations.
•The delta in speed (on GPU) between ConvNext and MobileViTv2 (R15-R18) at higher model
complexities reduces from 1:6to1:3when input resolution is increased from 224224(or
256256) to384384, suggesting ViT-based (including hybrid) models exhibit better scaling
properties as compared to CNNs. This is because of a higher degree of parallelism that ViT-based
models offer at a large scale [ 1,38]. Our results on down-stream tasks in Section 4.2 and previous
work on scaling ViTs [1, 39] further supports this observation.
4.2 Evaluation on down-stream tasks
Semantic segmentation We integrate MobileViTv2 with two standard segmentation architectures,
PSPNet [ 40] and DeepLabv3 [ 41], and study it on two standard semantic segmentation datasets,
ADE20k [ 42] and PASCAL VOC 2012 [ 43]. For training details including hyper-parameters, see
supplementary material.
Table 3 and Fig. 2c compares the segmentation performance in terms of validation mean intersection
over union (mIOU) of MobileViTv2 with different segmentation methods. MobileViTv2 delivers
competitive performance at different complexities while having signiﬁcantly fewer parameters and
FLOPs. Interestingly, the inference speed of MobileViTv2 models is comparable to CNN-based
models, including light-weight MobileNetv2 and heavy-weight ResNet-50 [ 44] model. This is
consistent with our observation in Section 4.1 (R17 vs. R18; Table 2) where we also observe that
ViT-based models scale better than CNN’s at higher input resolutions and model complexities.
Object detection We integrate MobileViTv2 with SSDLite [ 24] (SSD head [ 46] with separable
convolutions) for mobile object detection, and study its performance on MS-COCO dataset [ 47].
We follow [ 4] for training detection models. Table 4 and Fig. 2b compares SSDLite’s detection
performance in terms of validation mean average precision (mAP) using different ImageNet-1k
backbones. MobileViTv2 delivers competitive performance to models with different capacities,
further validating the effectiveness of the proposed self-separable attention method.
5 Visualizations of self-separable attention scores
Fig. 5 visualizes what the context scores learn at different output strides2ofMobileViTv2 network.
We found that separable self-attention layers pay attention to low-, mid-, and high-level features, and
allow MobileViTv2 to learn representations from semantically relevant image regions.
2Output stride is the ratio of the spatial dimension of the input to the feature map.
8
--- Page 9 ---
Table 4: Object detection using SSDLite on the MS-COCO dataset. Here, throughput is measured
with a batch of 128 images on the NVIDIA V100 GPU, and is an average over 50 iterations (excluding
10 iterations for warmup). Latency on a mobile device is not reported as some operations (e.g., hard
swish) are not optimally implemented for such devices. Rows are grouped by network parameters.
ImageNet-1k Image Throughput "" # Params## FLOPs##mAP""backbone Size (images/sec) (in millions) (in billions)
MobileViTv1-XXS 32022246 1.7 M 0.9 G 19.9
MobileViTv2 -0.5 (Ours) 32022782 2.0 M 0.9 G 21.2
MobileViTv2 -0.75 (Ours) 32021876 3.6 M 1.8 G 24.6
Mobilenetv2 32023052 4.3 M 0.8 G 22.1
MobileNetv3 32023884 5.0 M 0.6 G 22.0
MobileNetv1 32024330 5.1 M 1.3 G 22.2
MobileViTv2 -1.75 3202780 14.9 M 9.0 G 29.5
ResNet-50 3002744 22.9 M 20.2 G 25.2
Figure 5: Context score maps at different output strides (OS) of MobileViTv2 model. Observe
how context scores pay attention to semantically relevant image regions. ( Left to right: input image,
context scores at OS=8, context scores at OS=16, and context scores at OS=32). For more examples
and details about context score map generation, see Appendix D.
6 Conclusions
Transformer-based vision models are slow on mobile devices as compared to CNN-based models
because multi-headed self-attention is expensive on resource-constrained devices. In this paper, we
introduce a separable self-attention method that has linear complexity and can be implemented using
hardware-friendly element-wise operations. Experimental results on standard datasets and tasks
demonstrate the effectiveness of the proposed method over multi-headed self-attention.
Acknowledgements
We are grateful to Ali Farhadi, Peter Zatloukal, Oncel Tuzel, Rick Chang, Fartash Faghri, Farzad
Abdolhosseini, Lailin Chen, and Max Horton for their helpful comments. We are also thankful to
Apple’s infrastructure and open-source teams for their help with training infrastructure and open-
source release of the code and pre-trained models.
References
[1]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In
International Conference on Learning Representations , 2021. URL https://openreview.net/forum?
id=YicbFdNTTy .
9
--- Page 10 ---
[2]Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.
Training data-efﬁcient image transformers & distillation through attention. In International Conference on
Machine Learning , pages 10347–10357. PMLR, 2021.
[3]Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 10012–10022, 2021.
[4]Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly
vision transformer. In International Conference on Learning Representations , 2022. URL https:
//openreview.net/forum?id=vh-0sUt8HlG .
[5]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems ,
30, 2017.
[6]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
[7]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
[8]Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. arXiv preprint
arXiv:2001.04451 , 2020.
[9]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 , 2020.
[10] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear
complexity. arXiv preprint arXiv:2006.04768 , 2020.
[11] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin
Tran. Image transformer. In International Conference on Machine Learning , pages 4055–4064. PMLR,
2018.
[12] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-
attention for long document understanding. arXiv preprint arXiv:1911.02972 , 2019.
[13] Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention.
Advances in Neural Information Processing Systems , 33:21665–21674, 2020.
[14] Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing
Liu. Cluster-former: Clustering-based sparse transformer for question answering. In Findings of the
Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 3958–3968, 2021.
[15] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,
Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.
arXiv preprint arXiv:2009.14794 , 2020.
[16] Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Delight:
Deep and light-weight transformer. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=ujmgfuxSLrO .
[17] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference
on Computer Vision , pages 22–31, 2021.
[18] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.
Rethinking spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 11936–11945, 2021.
[19] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner:
Adaptive space-time tokenization for videos. Advances in Neural Information Processing Systems , 34,
2021.
[20] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and
Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pages 568–578, 2021.
10
--- Page 11 ---
[21] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training.
InInternational Conference on Learning Representations , 2018. URL https://openreview.net/
forum?id=r1gs9JgRZ .
[22] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantiza-
tion. In International Conference on Learning Representations , 2022. URL https://openreview.net/
forum?id=shpkpVXzo3h .
[23] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021.
URL https://arxiv.org/abs/2106.04560 .
[24] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 4510–4520, 2018.
[25] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pages 1314–1324, 2019.
[26] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. arXiv preprint arXiv:1409.0473 , 2014.
[27] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861 , 2017.
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .
[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision , 115(3):211–252, 2015.
[30] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations , 2017.
[31] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM
journal on control and optimization , 30(4):838–855, 1992.
[32] Sachin Mehta, Farzad Abdolhosseini, and Mohammad Rastegari. Cvnets: High performance library for
computer vision. CoRR , 2022.
[33] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the
masses. arXiv preprint arXiv:2104.10972 , 2021.
[34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
convnet for the 2020s. arXiv preprint arXiv:2201.03545 , 2022.
[35] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu.
Mobile-former: Bridging mobilenet and transformer. arXiv preprint arXiv:2108.05895 , 2021.
[36] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning , pages 6105–6114. PMLR, 2019.
[37] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for
efﬁcient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV) ,
pages 116–131, 2018.
[38] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
[39] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. CoRR ,
abs/2106.04560, 2021.
[40] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
network. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
2881–2890, 2017.
11
--- Page 12 ---
[41] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution
for semantic image segmentation. arXiv preprint arXiv:1706.05587 , 2017.
[42] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 633–641, 2017.
[43] Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes challenge: A retrospective. International journal of computer vision , 111
(1):98–136, 2015.
[44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.
[45] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and
benchmark. https://github.com/open-mmlab/mmsegmentation , 2020.
[46] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and
Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision , pages
21–37. Springer, 2016.
[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on
computer vision , pages 740–755. Springer, 2014.
[48] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function
approximation in reinforcement learning. Neural Networks , 107:3–11, 2018.
[49] Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A light-weight,
power efﬁcient, and general purpose convolutional neural network. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 9190–9200, 2019.
[50] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng,
and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In
Proceedings of the IEEE/CVF international conference on computer vision , 2021.
[51] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossVit: Cross-attention multi-scale vision transformer
for image classiﬁcation. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) , 2021.
[52] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to
vision transformers. arXiv preprint arXiv:2104.05707 , 2021.
[53] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:
Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697 ,
2021.
[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pages 1–9, 2015.
[55] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops , pages 702–703, 2020.
[56] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the
IEEE/CVF international conference on computer vision , pages 6023–6032, 2019.
[57] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. arXiv preprint arXiv:1710.09412 , 2017.
[58] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.
InProceedings of the AAAI conference on artiﬁcial intelligence , 2020.
[59] Ross Wightman, Hugo Touvron, and Hervé Jégou. Resnet strikes back: An improved training procedure in
timm. arXiv preprint arXiv:2110.00476 , 2021.
12
--- Page 13 ---
A Detailed architecture of MobileViTv2
MobileViTv2 ’s architecture follows MobileViTv1 [ 4] and is given in Table 5. MobileViTv2 block, shown
in Fig. 6, makes two changes to the MobileViTv1 block: (1) it replaces the multi-headed self-attention with
the proposed separable self-attention to learn global representations and (2) it does not use fusion block and
skip-connection (see Fig. 1b in [ 4]) as they improve the performance marginally (see Fig. 12 in [ 4]). The
expansion factor in MobileNetv2 [ 24] blocks and feed-forward layers is two. Similar to [ 4], we use Swish [ 48]
as a non-linear activation function. Unlike MobileViTv1 that creates three speciﬁc architectures (XXS, XS,
and S) for mobile devices, we uniformly scale the width of MobileViTv2 network using a width multiplier
20:5;2:0to create models at different complexities.
BMobileViTv2 ’s classiﬁcation performance
ImageNet-1k Table 6 shows the results of MobileViTv2 on the ImageNet-1k dataset. Finetuning
MobileViTv2 models at higher resolution ( 384384) shows improvement across the board. For example, the
performance of MobileViTv2 -0.50 with 1.4 million parameters improves by about 2% when ﬁnetuned at higher
resolution (R1 vs. R2). Similarly, pre-training on the ImageNet-21k-P dataset helps improve the performance of
MobileViTv2 models. For example, ImageNet-21k-P pretraining improves the performance of MobileViTv2 -
2.0 improves by 1.2% (R17 vs. R18). Notably, MobileViTv2 models pretrained on the ImageNet-21k-P are
able to achieve the similar performance with fewer FLOPs to models ﬁnetuned on ImageNet-1k with a higher
resolution (e.g., R10 vs. R11; R14 vs. R15; R18 vs. R19 in Table 6).
ImageNet-21k-P Table 7 shows the results on the ImageNet-21k-P validation dataset. The performance of
MobileViTv2 improves with increase in model size.
xDepth-wiseconvPoint-wiseconvUnfoldSeparable
self-attentionLFeed-forwardnetworkL
FoldPoint-wiseconv yB
Figure 6: MobileViTv2 block . Here, depth-wise convolution uses a kernel size of 33to encode
local representations. Similar to [ 4], unfolding and folding operations uses a patch height and width
of two respectively. The separable self-attention and feed-forward layers are repeated Bbefore
applying the folding operation.
Table 5: MobileViTv2 architecture. Here,drepresents dimensionality of the input to the separable
self-attention layer, Bdenotes the repetition of transformer block with separable self-attention inside
theMobileViTv2 block (Fig. 6), and MV2 indicates MobileNetv2 block. Similar to MobileViTv1
block, we set kernel size as three and spatial dimensions of patch (height hand widthw) as two in
theMobileViTv2 block.
Layer Output size Output stride Repeat Output channels
Image 256256 1
Conv- 33,#2128128 21 32
MV2 1 64
MV2,#26464 41 128
MV2 2 128
MV2,#23232 81 256
MobileViTv2 block (Fig. 6; B= 2) 1 256(d= 128)
MV2,#21616 161 384
MobileViTv2 block (Fig. 6; B= 4) 1 384(d= 192)
MV2,#2
88 321 512
MobileViTv2 block (Fig. 6; B= 3) 1 512(d= 256)
Global pool11 256 1512
Linear 1000
13
--- Page 14 ---
Table 6: Classiﬁcation performance of MobileViTv2 on the ImageNet-1k dataset. Here,yindi-
cates ﬁnetuning at higher resolution.
Row # Model Image size Extra data # Params ## FLOPs##Top-1""
R1 MobileViTv2 -0.50 2562None 1.4 M 0.5 G 70.2
R2 MobileViTv2 -0.50y3842None 1.4 M 1.0 G 72.1
R3 MobileViTv2 -0.75 2562None 2.9 M 1.0 G 75.6
R4 MobileViTv2 -0.75y3842None 2.9 M 2.3 G 77.0
R5 MobileViTv2 -1.00 2562None 4.9 M 1.8 G 78.1
R6 MobileViTv2 -1.00y3842None 4.9 M 4.1 G 79.7
R7 MobileViTv2 -1.25 2562None 7.5 M 2.8 G 79.6
R8 MobileViTv2 -1.25y3842None 7.5 M 6.3 G 80.9
R9 MobileViTv2 -1.50 2562None 10.6 M 4.0 G 80.4
R10 MobileViTv2 -1.50 2562ImageNet-21k-P 10.6 M 4.0 G 81.5
R11 MobileViTv2 -1.50y3842None 10.6 M 9.1 G 81.5
R12 MobileViTv2 -1.50y3842ImageNet-21k-P 10.6 M 9.1 G 82.6
R13 MobileViTv2 -1.75 2562None 14.3 M 5.5 G 80.8
R14 MobileViTv2 -1.75 2562ImageNet-21k-P 14.3 M 5.5 G 81.9
R15 MobileViTv2 -1.75y3842None 14.3 M 12.3 G 82.0
R16 MobileViTv2 -1.75y3842ImageNet-21k-P 14.3 M 12.3 G 82.9
R17 MobileViTv2 -2.00 2562None 18.5 M 7.2 G 81.2
R18 MobileViTv2 -1.75 2562ImageNet-21k-P 18.5 M 7.2 G 82.4
R19 MobileViTv2 -2.00y3842None 18.5 M 16.1 G 82.2
R20 MobileViTv2 -1.50y3842ImageNet-21k-P 18.5 M 16.1 G 83.4
Table 7: Performance of MobileViTv2 on the ImageNet-21k-P validation set.
Width factor # Params##FLOPs##Top-1""Top-5""
1.50 17.9 M 4.1 G 44.5 74.5
1.75 22.7 M 5.5 G 45.8 75.8
2.00 28.1 M 7.2 G 46.4 76.6
0 1 2 3 4 5 6 7 8
# Parameters (in million)556065707580T op-1 accuracy (%)
Mobilenetv1
MobileNetv2
MobileNetv3
ShuffleNetv2
ESPNetv2
MobileViTv2 (Ours)
(a) Comparison with light-weight CNNs
2 4 6 8 10
# Parameters (in million)646668707274767880T op-1 accuracy (%)
DeIT
ConViT
Mobile-former
CrossViT
LocalViT
T2T
MobileViTv2 (Ours) (b) Comparison with light-weight ViTs
Figure 7: Comparison with light-weight CNN- and ViT-based models. MobileViTv2 is smaller
and better, which is desirable for mobile devices.
C Comparisons with light-weight networks on the ImageNet-1k dataset
Comparison with light-weight CNNs. Fig. 7a shows that MobileViTv2 outperforms light-weight CNNs
across different network sizes (MobileNetv1 [ 27], MobileNetv2 [ 24], ShufﬂeNetv2 [ 37], ESPNetv2 [ 49], and
MobileNetv3 [25]).
Comparison with light-weight ViTs. Fig. 7b shows that MobileViTv2 achieves better performance than
previous light-weight ViT-based models acorss different network sizes (DeIT [ 2], T2T [ 50], CrossViT [ 51],
LocalViT [52], ConViT [53], and Mobile-former [35]).
D Visualizations of separable self-attention scores
TheMobileViTv2 block, Fig. 6, unfolds the input x2RdHWto obtain xu2Rd;MN, whereN=HW
hware the number of patches, each patch with width wand heighth(M=hwpixels per patch). This unfolded
feature map is fed to separable self-attention module to learn non-local representations. To better understand
how separable self-attention processes xu, we visualize context scores cs.
14
--- Page 15 ---
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(a)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(b)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(c)
Input Image
Context score map cmat an output stride of 8
Context score map cmat an output stride of 16
Context score map cmat an output stride of 32
(d)
Figure 8: Layer-wise visualization of context score maps cmat different output strides. Recall that
MobileViTv2 (Fig. 6 and Table 5) applies B= 2,B= 4, andB= 3separable self-attention layers
at an output strides of 8, 16, and 32 respectively. Therefore, we have 2, 4, and 8 context score maps
at an output stride of 8, 16, and 32 respectively
15
--- Page 16 ---
The separable self-attention in MobileViTv2 block computes context scores csforMpixels simultaneously
acrossNpatches. Therefore, cshas a dimensions of MN. To visualize context scores, we fold cs2RMN
to the same spatial dimensions as the input and obtain context score map cm2RHW. For ease of visualization,
we scale cmusing min-max normalization.
The context score maps for different input images at different output strides of MobileViTv2 model are
shown in Fig. 8. These visualizations show that the proposed separable self-attention method is able to (1)
aggregate information from entire image under different settings, including complex backgrounds, illumination
& view-point changes, and different objects, and (2) learn high-, mid-, and low-level representations.
EMobileViTv2 ’s ablation studies on the ImageNet-1k dataset
In this section, we study the effect on different methods on the performance of MobileViTv2 models, including
augmentation methods.
Standard vs. advanced augmentation We study two different augmentation methods: (1) standard
augmentation that uses Inception-style augmentation [ 54], i.e., random resized cropping and horizontal ﬂipping
and (2) advanced augmentation that uses RandAugment [ 55], CutMix [ 56], MixUp [ 57], and RandomErase
[58] along with standard augmentation methods. The effect of these augmentations on the performance
ofMobileViTv2 is shown in Figure 9. Smaller models ( <4:5million parameters) beneﬁt from standard
augmentation while larger models ( 4:5million parameters) beneﬁt from advanced augmentation. For
simplicity, we use advanced augmentation for all variants of MobileViTv2 in this paper.
Loss functions CutMix and Mixup augmentations mixes the samples in a batch. As a result, each sample
has multiple labels. Therefore, in presence of these augmentations, ImageNet classiﬁcation can be thought as a
multi-label classiﬁcation task. Similar to [ 59], we trained MobileViTv2 by minimizing binary cross-entropy
loss. Unlike [ 59], we did not observe any improvements in the performance when cross-entropy loss with label
smoothing is replaced with binary cross-entropy loss. Therefore, we use cross-entropy with label smoothing for
training MobileViTv2 models.
Effect of multiple latent tokens Similar to multi-head attention in transformers, the proposed separable
self-attention can have multiple latent tokens. When we changed the number of latent tokens from 1to8, the
performance improvements on the ImageNet-1k dataset were negligible (within 0:1top-1 accuracy). Therefore,
we use only one latent token in our experiments.
We note that changing the number of heads from 4to1in multi-headed self-attention in the transformer block of
the MobileViTv1-S architecture dropped the top-1 accuracy by 0.7%. This observation is similar to Vaswani
et al. [5], who also found that multiple heads in multi-headed self-attention improve transformers performance
on the task of neural machine translation.
Improving FLOP-efﬁciency via pixel- and patch-sampling The MobileViTv1 model [ 4] unfolds an
input feature map into Npatches, each patch with M=hwpixels and applies a transformer block for each
pixel in a patch independently, where handware patch’s height and width respectively. Because pixels in a
patch are spatially correlated, one can sub-sample mpixels fromMpixels and learn non-local representations
by applying self-attention layers on mpixels only. Such sub-sampling methods should help in reducing model
FLOPs.
0.5 2.5 4.5 6.5 8.510.5 12.5 14.5 16.5 18.5
Parameters (in millions)70727476788082T op-1 accuracy (in %)
Standard
Advanced
Figure 9: Impact of data augmentation on the performance of MobileViTv2 models on the
ImageNet-1k dataset. For smaller models ( <4:5million parameters), standard augmentation works
best while larger models ( 4:5million parameters) beneﬁt from advanced augmentation.
16
--- Page 17 ---
We tried following sampling methods at pixel- as well as patch-level:
•Random sampling , whereinmpixels (ornpatches) from Mpixels (orNpatches) are randomly selected
during training and uniformly during validation.
•Top-m(or top-n) sampling , wherein top- mpixels (or top- npatches) are selected based on their magnitude
computed using L2 norm.
•Uniform sampling , whereinmpixels (ornpatches) are sampled uniformly from Mpixels (orNpatches).
We found that these methods can reduce the FLOPs by 1:2to1:6with little or no drop in top-1 accuracy
on the ImageNet-1k dataset for both MobileViTv1 (with multi-headed self-attention) and MobileViTv2 (with
the proposed separable self-attention) models. However, these improvements in FLOPs did not translate to
latency improvements on a mobile device. In fact, models with these sampling methods were signiﬁcantly slower
than the models without these methods. The high-latency of models with these sampling methods on mobile
devices can be attributed to their high memory access cost, as these methods change the memory order of tensor.
Because of their high-latency on mobile devices, we did not use these methods in the MobileViTv2 model.
FMobileViTv2 training conﬁgurations
Conﬁgurations for training and ﬁnetuning MobileViTv2 -2.0 on the ImageNet-1k and ImageNet-21k-P datasets
are given in Table 8 and Table 9 respectively while conﬁgurations for ﬁnetuning MobileViTv2 on downstream
tasks are given in Table 10.
Training conﬁg MobileViTv2 -2.0
Dataset ImageNet-1k ImageNet-21k-P
# Training samples 1.28 M 11 M
# Validation samples 50 k 523 ky
Train resolution 256256 256256
Val resolution 256256 256256
RandAug 3 3
CutMix 3 3
MixUp 3 3
Random resized crop 3 3
Random horizontal ﬂip 3 3
Random erase 3 3
Stochastic depth 7 7
Label smoothing 3 3
Loss CE CE
Optimizer AdamW AdamW
Weight decay 0.05 0.05
Scheduler Cosine Cosine
Warm-up iterations 20 k None
Warm-up init LR 1e 6None
Warm-up scheduler Linear None
Base LR 0.002 0.0003
Epochs 300 80
Batch size 1024 4096
Layer-wise LR decay 7 7
Grad. clip 10 10
Exp. moving average 3 3
Weight init Random ImageNet-1k
Table 8: Conﬁguration for training MobileViTv2 -2.0 on the ImageNet-1k/22k-P datasets.yThe
validation set in ImageNet-21k-P does not overlap with ImageNet-1k validation set, and is created
following Ridnik et al. [33].
17
--- Page 18 ---
Training conﬁg MobileViTv2 -2.0
Dataset ImageNet-1k ImageNet-1k ImageNet-1k
# Training samples 1.28 M 1.28 M 1.28 M
# Validation samples 50 k 50 k 50 k
Train resolution 384384 256256 384 384
Val resolution 384384 256256 384 384
Weight init ImageNet-1k ImageNet-21k-P ImageNet-21k-P-1ky
RandAug 7 3 3
CutMix 7 3 3
MixUp 7 3 3
Random resized crop 3 3 3
Random horizontal ﬂip 3 3 3
Random erase 7 3 3
Stochastic depth 7 7 7
Label smoothing 3 3 3
Loss CE CE CE
Optimizer SGD SGD SGD
Weight decay 4e 54e 54e 5
Scheduler Fixed Cosine Fixed
Warm-up iterations None None None
Warm-up init LR None None None
Warm-up scheduler None None None
Base LR 0.001 0.01 0.001
Epochs 10 50 10
Batch size 128 256 128
Layer-wise LR decay 7 7 7
Grad. clip 10 10 10
Exp. moving average 3 3 3
Table 9: Conﬁguration for ﬁnetuning MobileViTv2 -2.0 on the ImageNet-1k dataset. Here,ydenotes
that the ImageNet-21k-P model ﬁnetuned on the ImageNet-1k dataset at 256256image resolution
is used for initializing the weights.
Training conﬁg SSDLite- MobileViTv2 -1.75 DeepLabv3- MobileViTv2 -1.75 DeepLabv3- MobileViTv2 -1.75
Dataset MS-COCO ADE20k PASCAL VOC 2012
Extra Data None None COCO
Task Detection Segmentation Segmentation
# Training samples 117 k 20 k 128 k
# Validation samples 5 k 2 k 1.45 k
Train resolution 320320 512 512 512 512
Val resolution 320320 Shortest side 512 Shortest side 512
Weight init ImageNet-1k ImageNet-1k ImageNet-1k
SSD Cropping 3 7 7
Photometric distortion 3 3 3
Random horizontal ﬂip 3 3 3
Resize 3 7 7
Random short size resize 7 3 3
Random Crop 7 3 3
Random Gaussian blur 7 3 3
Random rotation 7 3 3
Loss Smooth L1 + CE CE CE
Optimizer AdamW SGD AdamW
Weight decay 0.05 1e 40.05
Scheduler Cosine Cosine Cosine
Warm-up iterations 500 None 500
Warm-up init LR 9e 5None 5e 5
Warm-up scheduler Linear None Linear
Base LR 0.0009 0.02 0.0005
Epochs 200 120 50
Batch size 128 16 128
Layer-wise LR decay 7 7 7
Grad. clip 10 10 10
Exp. moving average 3 3 3
Table 10: Conﬁguration for ﬁnetuning MobileViTv2 on downstream tasks. For Ade20k, we found
that SGD was more stable as compared to AdamW across different MobileViTv2 conﬁgurations,
and therefore, we used SGD for ﬁnetuning on Ade20k dataset. The conﬁgurations for MobileViTv2
with PSPNet are the same as Deeplabv3 on both PASCAL VOC and Ade20k datasets.
18
